\documentclass[11pt,a4paper,]{article}
\usepackage{lmodern}

\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdftitle={On combining forecasting methods using time series features},
            pdfkeywords={FFORMA (Feature-based FORecast-model Averaging), FFORMS (Feature-based
FORecast-model Selection), Time series features, Forecast combination,
XGBoost, M4 Competition},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{geometry}
\geometry{a4paper, text={16cm,24cm}}
\usepackage[style=authoryear-comp,]{biblatex}
\addbibresource{references.bib}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{long table}}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


\title{On combining forecasting methods using time series features}

%% MONASH STUFF

%% CAPTIONS
\RequirePackage{caption}
\DeclareCaptionStyle{italic}[justification=centering]
 {labelfont={bf},textfont={it},labelsep=colon}
\captionsetup[figure]{style=italic,format=hang,singlelinecheck=true}
\captionsetup[table]{style=italic,format=hang,singlelinecheck=true}

%% FONT
\RequirePackage{bera}
\RequirePackage{mathpazo}

%% HEADERS AND FOOTERS
\RequirePackage{fancyhdr}
\pagestyle{fancy}
\rfoot{\Large\sffamily\raisebox{-0.1cm}{\textbf{\thepage}}}
\makeatletter
\lhead{\textsf{\expandafter{\@title}}}
\makeatother
\rhead{}
\cfoot{}
\setlength{\headheight}{15pt}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\fancypagestyle{plain}{%
\fancyhf{} % clear all header and footer fields
\fancyfoot[C]{\sffamily\thepage} % except the center
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}}

%% MATHS
\RequirePackage{bm,amsmath}
\allowdisplaybreaks

%% GRAPHICS
\RequirePackage{graphicx}
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.8}

%\RequirePackage[section]{placeins}

%% SECTION TITLES
\RequirePackage[compact,sf,bf]{titlesec}
\titleformat{\section}[block]
  {\fontsize{15}{17}\bfseries\sffamily}
  {\thesection}
  {0.4em}{}
\titleformat{\subsection}[block]
  {\fontsize{12}{14}\bfseries\sffamily}
  {\thesubsection}
  {0.4em}{}
\titlespacing{\section}{0pt}{*5}{*1}
\titlespacing{\section}{0pt}{*2}{*0.2}


%% TITLE PAGE
\def\Date{\number\day}
\def\Month{\ifcase\month\or
 January\or February\or March\or April\or May\or June\or
 July\or August\or September\or October\or November\or December\fi}
\def\Year{\number\year}

\makeatletter
\def\wp#1{\gdef\@wp{#1}}\def\@wp{??/??}
\def\jel#1{\gdef\@jel{#1}}\def\@jel{??}
\def\showjel{{\large\textsf{\textbf{JEL classification:}}~\@jel}}
\def\nojel{\def\showjel{}}
\def\addresses#1{\gdef\@addresses{#1}}\def\@addresses{??}
\def\cover{{\sffamily\setcounter{page}{0}
        \thispagestyle{empty}%
        \vspace*{-2cm}
        \centerline{\raisebox{-1.8cm}{\includegraphics[width=5cm]{MBSportrait}}\hspace*{9cm} ISSN 1440-771X}\vspace{0.99cm}
        \begin{center}\Large
        Department of Econometrics and Business Statistics\\[.5cm]
        \scriptsize http://business.monash.edu/econometrics-and-business-statistics/research/publications
        \end{center}\vspace{2cm}
        \begin{center}
        \fbox{\parbox{14cm}{\begin{onehalfspace}\centering\Huge\vspace*{0.3cm}
                \textsf{\textbf{\expandafter{\@title}}}\vspace{1cm}\par
                \LARGE\@author\end{onehalfspace}
        }}
        \end{center}
        \vfill
                \begin{center}\Large
                \Month~\Year\\[1cm]
                Working Paper \@wp
        \end{center}}}
\def\pageone{{\sffamily
        \newpage%\blankpage
        \thispagestyle{empty}
        \vbox to 23cm{
        \raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}
        \vspace{2cm}\par
        \hspace{1cm}\parbox{14cm}{\sffamily\large\@addresses}\vspace{1cm}\vfill
        \hspace{1cm}{\large\Date~\Month~\Year}\\[1cm]
        \hspace{1cm}\showjel\vss}}}
\def\blindtitle{{\sffamily
     \thispagestyle{plain}\raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}\vspace{1cm}\par
        }}
\def\titlepage{{\cover\newpage\setstretch{1}\pageone\newpage\blindtitle}}

\def\blind{\def\titlepage{{\blindtitle}}\let\maketitle\blindtitle}
\def\titlepageonly{\def\titlepage{{\pageone\end{document}}}}
\def\nocover{\def\titlepage{{\newpage\setstretch{1}\pageone\newpage\blindtitle}}\let\maketitle\titlepage}
\let\maketitle\titlepage
\makeatother

%% SPACING
\RequirePackage{setspace}
\spacing{1.5}

%% LINE AND PAGE BREAKING
\sloppy
\clubpenalty = 10000
\widowpenalty = 10000
\brokenpenalty = 10000
\RequirePackage{microtype}

%% PARAGRAPH BREAKS
\setlength{\parskip}{1.4ex}
\setlength{\parindent}{0em}

%% HYPERLINKS
\RequirePackage{xcolor} % Needed for links
\definecolor{darkblue}{rgb}{0,0,.6}
\RequirePackage{url}

\makeatletter
\@ifpackageloaded{hyperref}{}{\RequirePackage{hyperref}}
\makeatother
\hypersetup{
     citecolor=0 0 0,
     breaklinks=true,
     bookmarksopen=true,
     bookmarksnumbered=true,
     linkcolor=darkblue,
     urlcolor=blue,
     citecolor=darkblue,
     colorlinks=true}

%% KEYWORDS
\newenvironment{keywords}{\par\vspace{0.5cm}\noindent{\sffamily\textbf{Keywords:}}}{\vspace{0.25cm}\par\hrule\vspace{0.5cm}\par}

%% ABSTRACT
\renewenvironment{abstract}{\begin{minipage}{\textwidth}\parskip=1.4ex\noindent
\hrule\vspace{0.1cm}\par{\sffamily\textbf{\abstractname}}\newline}
  {\end{minipage}}


\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage[showonlyrefs]{mathtools}
\usepackage[no-weekday]{eukdate}

%% BIBLIOGRAPHY

\makeatletter
\@ifpackageloaded{biblatex}{}{\usepackage[style=authoryear-comp, backend=biber, natbib=true]{biblatex}}
\makeatother
\ExecuteBibliographyOptions{bibencoding=utf8,minnames=1,maxnames=3, maxbibnames=99,dashed=false,terseinits=true,giveninits=true,uniquename=false,uniquelist=false,doi=false, isbn=false,url=true,sortcites=false}

\DeclareFieldFormat{url}{\texttt{\url{#1}}}
\DeclareFieldFormat[article]{pages}{#1}
\DeclareFieldFormat[inproceedings]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[incollection]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[article]{volume}{\mkbibbold{#1}}
\DeclareFieldFormat[article]{number}{\mkbibparens{#1}}
\DeclareFieldFormat[article]{title}{\MakeCapital{#1}}
\DeclareFieldFormat[article]{url}{}
%\DeclareFieldFormat[book]{url}{}
%\DeclareFieldFormat[inbook]{url}{}
%\DeclareFieldFormat[incollection]{url}{}
%\DeclareFieldFormat[inproceedings]{url}{}
\DeclareFieldFormat[inproceedings]{title}{#1}
\DeclareFieldFormat{shorthandwidth}{#1}
%\DeclareFieldFormat{extrayear}{}
% No dot before number of articles
\usepackage{xpatch}
\xpatchbibmacro{volume+number+eid}{\setunit*{\adddot}}{}{}{}
% Remove In: for an article.
\renewbibmacro{in:}{%
  \ifentrytype{article}{}{%
  \printtext{\bibstring{in}\intitlepunct}}}

\AtEveryBibitem{\clearfield{month}}
\AtEveryCitekey{\clearfield{month}}

\makeatletter
\DeclareDelimFormat[cbx@textcite]{nameyeardelim}{\addspace}
\makeatother
\renewcommand*{\finalnamedelim}{%
  %\ifnumgreater{\value{liststop}}{2}{\finalandcomma}{}% there really should be no funny Oxford comma business here
  \addspace\&\space}
  

\wp{no/yr}
\jel{C10,C14,C22}


\blind



\date{\sf\Date~\Month~\Year}
\makeatletter
 \lfoot{\sf\@date}
\makeatother

%% Any special functions or other packages can be loaded here.
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsthm}
\usepackage{amsmath,bm}
\usepackage{paralist}
\usepackage{todonotes}
\usepackage{ctable}

\def\sectionautorefname{Section}
\captionsetup[figure]{font=small}
\captionsetup[table]{font=small}
\def\var{\text{Var}}
\allowdisplaybreaks
\sloppy

%% LINE AND PAGE BREAKING
\clubpenalty = 4500
\widowpenalty = 4500
\brokenpenalty = 4500


\def\yes{$\checkmark$}

\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{5pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}


\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{conjecture}{Conjecture}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[section]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle
\begin{abstract}
It is well known that ensemble approaches produce improvements over
single methods in statistical learning. Nevertheless, when calculating
predictions over a large dataset, computation time for the whole
ensemble can be prohibitive, so individual model selection becomes the
preferred approach. We present a method for combining forecasting models
by posing it as a classification problem using features extracted from
the time series. Unlike regular classification problems, we minimize the
average forecast error of the selected method rather than the
classification error. Not only does this address the aim of accurate
forecasting, it also provides measures of relative method accuracy
across the time series, and relative difficulty across time series. In
contrast, a classic classification approach would give the same
importance to all series and methods. The presented classifier is
compared with state-of-the-art approaches to forecasting and time series
classification. The results show an improvement of error over
alternative approaches. These experiments allow us to show the relevance
of both the feature set and the proposed optimization approach to
several collections of time series. The scalability of the approach
allows it to be applied to forecasting a large collection of time
series. It can also be efficiently trained to tailor specific domains or
datasets.
\end{abstract}
\begin{keywords}
FFORMA (Feature-based FORecast-model Averaging), FFORMS (Feature-based
FORecast-model Selection), Time series features, Forecast combination,
XGBoost, M4 Competition
\end{keywords}

\section{Introduction}\label{introduction}

There are essentially two general approaches to forecast a time series:
i) use of single model and ii) combination forecast or forecast model
averaging. There is a growing consensus that the combination forecast is
a way of improving forecast accuracy. Empirical work often provides
evidence that the combination of forecasts resulting from several
candidate forecasting models are often superior to their individual
forecasts. Combining forecasts across different models is considered as
a way of reducing the risk of selecting an inappropriate model. However,
the main challenge in forecast combination is the selection of
appropriate set of weights.

Granger (1969), was the first to forward the idea of ``the combination
of forecasts''. Since then many approaches have been proposed to derive
weights for forecast combination (Timmerman, 2006). Simple
averages(ref\_clemen1989), regression-based approaches, \ldots{} are
name to few.

Recently, a few researchers have explored the use of time series
features in selecting the most appropriate forecasting method. However,
use of time series features to derive weights for forecast combination
has been rarely addressed in the literature. In this paper we propose a
general framework to obtain weights for forecast combination based on
features computed from the time series. We call this framework
FFORMA(Feature-based FORecast Model Averaging). The proposed FFORMA
framework has been used over the course of M4 competition and placed in
second in forecast accuracy in both point forecasts and prediction
intervals.

\emph{Highligh simplicity of the forecsint methods and no fine tuning
(long series only last part, models per type weekly, daily,etc..)}
\emph{No exogenous information, such as macroeconimic, finance, tourism}

** EXAMPLE **

\begin{itemize}
\tightlist
\item
  \textbf{Contribution LIST / Refutable} *
\end{itemize}

The rest of the paper is organized as follows. Section 3 presents the
methodology underlying the FFORMA framework followed by Section 4 that
describes the application of the framework to M4 competition, with
specific decisions as the set of features, individual forecasting
methods, and learning algortithms. \emph{The method to produce the
prediction intervals for the M4 competitionis also explained in this
section.}

Finally Section 5 presents the conclusions and future work.

\section{Methodology}\label{methodology}

This section describes the rational behind the proposed framework. In
the proposed framework we use meta-learning approach to derive weights
for forecast combination. Figure 1 presents the Pseudo code of the
proposed framework. This approach has been influenced by the work of
\textcite{fforms}.

\subsection{Overview / Intuition}\label{overview-intuition}

The general objective of meta-learning is to learn to ``combine''
individidual forecasting methods to minimize forecasting error. We start
from a dataset of time series, which we call training set, which may
include the time series we want to forecast, and a set of forecasting
methods. The forecasting errors of each forecasting method on each time
series are calculated, either using the true future values of the series
if available or a temporal holdout version of the series.

The meta-learning model is then fitted on the training set, essentially
trying to find how to combine the methods in our pool to minimize the
forecasting error on each series. The input for training the
meta-learning model are features extracted from the series (e.g.~length,
autocorrelation coefficients,\ldots{}) and the error that is produced by
each of the forecasting methods. Once the meta-learning model is
trained, the inductive step consist of assuming the series we want to
forecast comes from a ``generating process'' similar to the one that
generated the training set.

One common meta-learning approach is to learn to select the best method
for each series, the one that produces the least forecasting error. This
transforms the problem into a traditional classification problem and
allows the use of existing classifcation algorithms. However, this
simplification removes relevant information which may be useful in the
metalearning process, such as which methods producing similar error to
the best on for a series, or series that produce larger errors than
others. We do not apply this simplification and take into account the
exact error that each method produces in each series, instead of just
considering which one produces the minimum error. We then pose the
problem as finding a function that assigns probabilities to each
forecasting method for each series, with the goal of minimizing the
expected error that is produced if the methods were picked at random
following these probabilties. It can be seen that in the extreme, our
approach can be easily transformed to a classification exercise if we
pick the method with the largest probability, and that if we allow a
sufficiently rich hyphotesis space to fit freely, we would end up
assigning probability one to the methods with the least amount of error.

The meta-learning process can therefore be summarized as:

\begin{itemize}
\tightlist
\item
  \(X_i\) the individual series on the training set.
\item
  \(e_{ij}\) the error the individual forecasting method \(j\) makes on
  series \(i\).
\item
  \(x_{i}\) the feature vector extracted from series \(X_i\).
\item
  \(f\) is a function belonging to the hypothesis space \(H\) that maps
  features to probabilities of the \(j\) forecasting methods.
\end{itemize}

\[ argmin_{f \in H} \sum_i \sum_j f(x_{i})_je_{ij} \]

Given a new time series, a trained meta-learning algorithm produces
probabilities for each of the forecasting methods considered in the
pool. The final forecast can be computed from these probabilities in
two-ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A weighted average of the individual forecast methods. This tends to
  produce the best forecast on average, but requires computing the
  forecasts of all methods in out pool, which may be time consuming.
\item
  Selecting the method with the largest probability. This prevents the
  need to calculate all the forecast of the individual methods in our
  pool, but produces less acurate forecasts on average.
\end{enumerate}

\subsection{Algorithmic description}\label{algorithmic-description}

The FFORMA framework has four main components:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The set of forecasting methods, pool of methods.
\item
  The set of features to be extracted from the time series
\item
  A training set of time series, the reference set.
\item
  The forecasting error measure, such as mean squared error, .
\end{enumerate}

The FFORMA framework works in two phases: an \textbf{offline} phase and
an \textbf{online} phase.

In the offline phase a meta-learner is trained using a diverse
collection of time series, called the reference set. Each time series in
the reference set is divided in to training period and test period.
Features are extracted based on the training period of each time series.
These features are one of the \textbf{two kinds} of inputs to the
meta-learner. The second input is the forecasting error produced by each
of the methods in the poo for each series. This error is calculated
comparing the forecast of the methods fitted in the training period to
the test period, using a error measure.

\begin{algorithm}[!ht]
  \caption{The FFORMA framework - Forecast combination based on meta-learning. }
  \label{alg:algo-lab}
  \begin{algorithmic}[1]
    \Statex \textbf{Offline phase - train the learning model}
    \Statex \text{Given:}
    \Statex \hspace{1cm}$O=\{X_1, X_2, \dots,X_n\}:$ the collection of $N$ observed time series.
      \Statex \hspace{1cm}$P:$ Set of $K$ forecasting algorithms such as ARIMA, ETS, SNAIVE, etc.
         \Statex \hspace{1cm}$F:$ the set of functions to calculate time series features.
         \Statex \hspace{1cm}$E:$ A forecasting error measure such as Mean Squared Error.
     \Statex \text{Output:}
      \Statex \hspace{1cm}\text{FFORMA meta-learner:}
      \Statex \text{A function from the extracted features to a set of $K$ probabilities, one for each method in $P$.}
    \Statex \textit{Prepare the meta-data}
    \Statex For $j=1$ to $N$:
            \State Split $X_j$ into a training period and test period.
            \State Calculate the set of features for the training period by applying $F$.
            \State Fit $P$ models to the training period.
            \State Calculate forecasts for the test period from each model.
            \State Calculate forecast error measure $E$ over the test period for all models in $P$.
            \State Meta-data: input features $x_i$ (step 4), output errors: $e_i$ (step 5).
     \Statex
    \Statex \textit{Train the meta-learner}
            \State Train a learning model based on the meta-data and errors, by minimizing:
            $$ argmin_f \sum_{i=1}^n \sum_{k=1}^K f(x_i)_k e_k  $$
            \State {meta-learner}.
    \Statex
     \Statex \textbf{Online phase - forecast a new time series}
    \Statex \text{Given:}
    \Statex \hspace{1cm}\text{FFORMA classifier from step 8} .
     \Statex \text{Output:}
      \Statex \hspace{1cm}\text{Forecast for the new time series $x_{new}$}.
  \State For $x_{new}$ calculate features $F$.
  \State From the features, use the meta-learner to produce $w$ the vector of probabilities. 
  \State Compute the individual forecasts of the methods in $P$
  \State Compute the final forecast by weighted average using $w$ and the individual forecasts.
   \end{algorithmic}
\end{algorithm}

The algorithm produces weights for each forecasting method. These
weights can also be interpreted as the probability of each method being
best. These weights can be used either to select the ``best''
forecasting method for each series or to combine the forecasts using
weighted linear combination. Note that the accuracy of the FFORMA
meta-learner depends on three main factors:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Forecasting methods used in the pool
\item
  The set of time series features we considered
\item
  The collection of time series used to train the classifier.
\end{enumerate}

Section 3 provides a more detailed description of application of the
FFORMA framework over the course of M4 competition. The proposed
framework is closely related to the previous work by ref\_prudencio
which they use machine learning techniques to define weights for the
linear combination of forecasts.

\section{FFORMA framework: Application to M4
Competition}\label{fforma-framework-application-to-m4-competition}

\subsection{Data preprocessing}\label{data-preprocessing}

The M4 competition database consists of 100,000 real-world time series
of yearly, quarterly, monthly, weekly, daily and hourly data. The
frequency for yearly, quarterly, monthly and weekly data are considered
as 1, 4, 12 and 52 respectively. {[}Q: Daily and Hourly series, what are
the frequencies considered?{]}. We used xxxx time series out of 100000
to train a meta-learner and rest is used to evaluate the proposed
framework. In addition to the time series provided in the M4 competition
database we used the time series of M1 and M3 competitions
\autocite{makridakis2000m3} to the reference set. Each time series in
the reference set is split into training period and test period. The
length of test period of each time series was set as according to the
rules of M4 competition, 6 for yearly data, 8 for quarterly, 18 for
monthly, 13 for weekly, 14 for daily and 48 for hourly.

\subsection{Time series features}\label{time-series-features}

Table xxx provides a detailed description of features used in this
experiment.

{[}include table{]}

These features have been previously used by \textcite{fforms} and
\textcite{hyndman2015large}. A detailed description of these features
are provided in \textcite{fforms}. All the features are implemented in
\texttt{tsfeatures} R package by xxx.

\textcolor{red}{Question: calculation of features for time series with multiple seasonality, and short time series?}

\subsection{Forecasting methods}\label{forecasting-methods}

We considered nine forecasting algorithms implemented in the forecast
package in R. They are(the specific R calls for fitting the models are
given): i) automated ARIMA algorithm (\texttt{auto.arima}), ii)
automated ETS algorithm (\texttt{ets}), iii) feed-forward neural network
with a single hidden layer is fitted to the lags. The number of lags is
automatically selected (\texttt{nnetar}) iv) random walk with drift
(\texttt{rwf} with drift=TRUE), v) TBATS model (\texttt{tbats}), vi)
Theta method forecast (\texttt{thetaf}), vii) naive forecasts
(\texttt{naive}) viii) STLM-AR Seasonal and Trend decomposition using
Loess with AR modeling of the seasonally adjusted series(\texttt{stlm}
with modelfunction \texttt{ar}), ix) seasonal naive forecasts
(\texttt{snaive}).
\textcolor{red}{In the case of an error when fitting the series (e.g. a series is constant), the SNAIVE forecast method is used instead.}

\textcolor{red}{Question: i) Calculation of ets models to daily, hourly and weekly series}

\subsection{Metal-learning model}\label{metal-learning-model}

We choose the gradient tree boosting of \textbf{xgboost} as the
underlying implementation of the learning model for the following
reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ability to customize the model to fit our objective function
\item
  Computational efficiency: The size of the M4 dataset requires a
  efficient model
\item
  Good performance in structure based problems
\end{enumerate}

\subsubsection{The xgboost objetive
function}\label{the-xgboost-objetive-function}

We can consider two kinds of inputs to our learning model, the features
and the errors of the individual methods in the pool. The xgboost
algorithm produces numeric values from the features, one for each
forecasting method in our pool. We apply the softmax transform to these
values and the compute our objective function (the loss function) using
these probabilities and the errors produced by the methods in our pool.

xgboost fits the model by gradient boosting, for which requires the
gradient and hessian of the objective function. In our case, the
gradient is the direct gradient of the objective, but the \emph{correct}
hessian is prone to numerical problems that need to be fixed for xgboost
to converge. This is a relative common problem and one simple fix is to
use an upper bound of the hessian by clampling small values to a larger
one. In our case, we compute an alternate upper bound of the hessian by
removing some terms from the \emph{correct} hessian . Although both
alternatives converge, our approach works much fastes, requiring less
boosting steps to converge. This not only increases the computational
efficiency, in our scenario it also generalizes better due to a less
complex set of trees produced in the final solution.

The functions involved in computing the objective are the following:

\begin{itemize}
\tightlist
\item
  \(y(x)\) is the output of the xgboost algorithm
\item
  \(p_j = \frac{e^{y(x)_j}}{ \sum_k e^{y(x)_k}}\) is the transformation
  to probabilities by applying the softmax transform.
\item
  \(L = \sum p_j e_j\) is the loss of the function \(p\).
\item
  \(G_j = \frac{\partial{L}}{\partial{p_j}} = p_j(e_j -\sum_k e_kp_k)\)
  \[ H_j = \frac{\partial{G_j}}{\partial{p_j}} \approx \hat{H}_j = p_j(e_j(1-p_j) - G_j)  \]
\end{itemize}

\subsubsection{Hyperparameters}\label{hyperparameters}

The results of xgboost are particularly dependent on its hyperparameters
such as learning rate, number of boosting steps, maximum complexity
allowed for the trees or subsampling sizes. In our case we limit the
hyperparameter search space based on some initial results and rules of
thumb and explore it using bayesian optimization, measuring performance
on a 10\% holdout version of the training set. We pick the most simple
hyperparameter set from the top solutions of the exploration.

\subsection{Generate point forecasts}\label{generate-point-forecasts}

\subsection{Prediction Intervals}\label{prediction-intervals}

The M4 Competition also featured a subcompetition over the accuracy of
prediction intervals. For this part of the competition, we used a
different approach that for the point forecasts. In order to compute the
intervals we used the point forecast produced by our meta-learner as the
centre of the interval and computed the 95\% bounds of the interval by a
linear combination of the bounds of three forecasting methods: Thetaf,
SNAIVE and NAIVE methods. The coefficients for the linear combination
were calculated in a data driven way. The procedure is as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For the training period each series in the dataset:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Compute the point forecast of the meta-learning.
  \item
    Compute the 95\% \emph{predicion radius} for the thetaf, snaive and
    naive, this is the difference between the 95\% upper bound and the
    point forecast for each horizon.
  \end{enumerate}
\item
  For each horizon in the dataset:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Find the coefficients that minimize the MSIS error of the interval
    with the meta-learning point forecast as center and a linear
    combination of the radiuses of thetaf, snaive and naive as radius.
    The coefficients are calculated by gradient descent.
  \end{enumerate}
\end{enumerate}

This procedure produces at set of three coefficients for each of the
prediction horizons in the M4 dataset and these coefficients will be the
same independently of the series we want to forecast. These coefficients
are also not restricted to be probabilities, the optimization is
unrestricted.

\section{Results (Do we need this?)}\label{results-do-we-need-this}

\textcolor{blue}{ Fotios email: We would like to ask that this paper focuses on the clear description of the method (and possibly include flowcharts, pseudocode, etc) without any empirical evidence (that would be part of the main M4-competition paper). }

Maybe we can show here results about the metalearning, how weights
evolve with series length. What is the difference between
weightedaveraging and selection in our model? What happens if we remove
individual forecasting methods? How robust are we against this? wHich
features are more important?

\section{Discussion and Conclusions}\label{discussion-and-conclusions}

\textcolor{blue}{Fotios email: We would like, though, to see a short section that discusses the reasons behind the good performance of your method. }

\subsection{Reasons behind the
performance}\label{reasons-behind-the-performance}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  As opposed to individual methods in our pool, it is being trained to
  the specific error in the M4 Competition. The methods in our pool do
  not minimize the OWA error, but the squared loss, while we generated
  probabilities based on the OWA error.
\item
  As opposed to individual selection methods, it exploits domain bias.
  One method could be discarded in a series when the error is low,
  because on average it performs bad in the rest of the dataset, while
  individual methods would pick it up. "
\item
  Exploiting dependencies between time series. In the M4 competition,
  some time series are similar, which can be hypothesized as
  macroeconomic indicators of neighbouring countries, etc. This effect
  is explited in the metalearning model and the series would produce
  similar features, and the errors that the model minimizes are more
  accurate.
\item
  AS opossed to traditional classification meta-learning approaches:
  Take the individual errors into account. This works two ways,
  capturing similarity the results in methods to keep the model simple
  (. Capturing differences in time series to keep the model simple
  (difficult do not end up affecting the model much).
\item
  Compared to a naive averaging of individual methods, it exploits
  easily discardable methods. Many ensemble approaches work by averaging
  the output of individual methods. This is especially when the number
  of individual forecasting methods grows and allows us to include
  methods with radically different assumptions that would not be
  appropiate to average. e.g.``for really short time series you really
  cant do better than naive, others just overfit''
\item
  As opposed to just holdout crossvalidation, our method performs
  better. If we allow the method to overfit, i will reproduce the
  results of the holddout crossvaliation error, but in our case it
  improves it.
\end{enumerate}

\printbibliography[title=References]

\end{document}
