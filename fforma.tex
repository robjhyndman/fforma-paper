\documentclass[11pt,a4paper,]{article}
\usepackage{lmodern}

\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdftitle={FFORMA: Feature-based FORecast-model Averaging},
            pdfkeywords={FFORMA (Feature-based FORecast-model Averaging), FFORMS (Feature-based
FORecast-model Selection), Time series features, Forecast combination,
XGBoost, M4 Competition},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{geometry}
\geometry{a4paper, text={16cm,24cm}}
\usepackage[style=authoryear-comp,]{biblatex}
\addbibresource{references.bib}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{long table}}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


\title{FFORMA: Feature-based FORecast-model Averaging}

%% MONASH STUFF

%% CAPTIONS
\RequirePackage{caption}
\DeclareCaptionStyle{italic}[justification=centering]
 {labelfont={bf},textfont={it},labelsep=colon}
\captionsetup[figure]{style=italic,format=hang,singlelinecheck=true}
\captionsetup[table]{style=italic,format=hang,singlelinecheck=true}

%% FONT
\RequirePackage{bera}
\RequirePackage{mathpazo}

%% HEADERS AND FOOTERS
\RequirePackage{fancyhdr}
\pagestyle{fancy}
\rfoot{\Large\sffamily\raisebox{-0.1cm}{\textbf{\thepage}}}
\makeatletter
\lhead{\textsf{\expandafter{\@title}}}
\makeatother
\rhead{}
\cfoot{}
\setlength{\headheight}{15pt}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\fancypagestyle{plain}{%
\fancyhf{} % clear all header and footer fields
\fancyfoot[C]{\sffamily\thepage} % except the center
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}}

%% MATHS
\RequirePackage{bm,amsmath}
\allowdisplaybreaks

%% GRAPHICS
\RequirePackage{graphicx}
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.8}

%\RequirePackage[section]{placeins}

%% SECTION TITLES
\RequirePackage[compact,sf,bf]{titlesec}
\titleformat{\section}[block]
  {\fontsize{15}{17}\bfseries\sffamily}
  {\thesection}
  {0.4em}{}
\titleformat{\subsection}[block]
  {\fontsize{12}{14}\bfseries\sffamily}
  {\thesubsection}
  {0.4em}{}
\titlespacing{\section}{0pt}{*5}{*1}
\titlespacing{\section}{0pt}{*2}{*0.2}


%% TITLE PAGE
\def\Date{\number\day}
\def\Month{\ifcase\month\or
 January\or February\or March\or April\or May\or June\or
 July\or August\or September\or October\or November\or December\fi}
\def\Year{\number\year}

\makeatletter
\def\wp#1{\gdef\@wp{#1}}\def\@wp{??/??}
\def\jel#1{\gdef\@jel{#1}}\def\@jel{??}
\def\showjel{{\large\textsf{\textbf{JEL classification:}}~\@jel}}
\def\nojel{\def\showjel{}}
\def\addresses#1{\gdef\@addresses{#1}}\def\@addresses{??}
\def\cover{{\sffamily\setcounter{page}{0}
        \thispagestyle{empty}%
        \vspace*{-2cm}
        \centerline{\raisebox{-1.8cm}{\includegraphics[width=5cm]{MBSportrait}}\hspace*{9cm} ISSN 1440-771X}\vspace{0.99cm}
        \begin{center}\Large
        Department of Econometrics and Business Statistics\\[.5cm]
        \scriptsize http://business.monash.edu/econometrics-and-business-statistics/research/publications
        \end{center}\vspace{2cm}
        \begin{center}
        \fbox{\parbox{14cm}{\begin{onehalfspace}\centering\Huge\vspace*{0.3cm}
                \textsf{\textbf{\expandafter{\@title}}}\vspace{1cm}\par
                \LARGE\@author\end{onehalfspace}
        }}
        \end{center}
        \vfill
                \begin{center}\Large
                \Month~\Year\\[1cm]
                Working Paper \@wp
        \end{center}}}
\def\pageone{{\sffamily
        \newpage%\blankpage
        \thispagestyle{empty}
        \vbox to 23cm{
        \raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}
        \vspace{2cm}\par
        \hspace{1cm}\parbox{14cm}{\sffamily\large\@addresses}\vspace{1cm}\vfill
        \hspace{1cm}{\large\Date~\Month~\Year}\\[1cm]
        \hspace{1cm}\showjel\vss}}}
\def\blindtitle{{\sffamily
     \thispagestyle{plain}\raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}\vspace{1cm}\par
        }}
\def\titlepage{{\cover\newpage\setstretch{1}\pageone\newpage\blindtitle}}

\def\blind{\def\titlepage{{\blindtitle}}\let\maketitle\blindtitle}
\def\titlepageonly{\def\titlepage{{\pageone\end{document}}}}
\def\nocover{\def\titlepage{{\newpage\setstretch{1}\pageone\newpage\blindtitle}}\let\maketitle\titlepage}
\let\maketitle\titlepage
\makeatother

%% SPACING
\RequirePackage{setspace}
\spacing{1.5}

%% LINE AND PAGE BREAKING
\sloppy
\clubpenalty = 10000
\widowpenalty = 10000
\brokenpenalty = 10000
\RequirePackage{microtype}

%% PARAGRAPH BREAKS
\setlength{\parskip}{1.4ex}
\setlength{\parindent}{0em}

%% HYPERLINKS
\RequirePackage{xcolor} % Needed for links
\definecolor{darkblue}{rgb}{0,0,.6}
\RequirePackage{url}

\makeatletter
\@ifpackageloaded{hyperref}{}{\RequirePackage{hyperref}}
\makeatother
\hypersetup{
     citecolor=0 0 0,
     breaklinks=true,
     bookmarksopen=true,
     bookmarksnumbered=true,
     linkcolor=darkblue,
     urlcolor=blue,
     citecolor=darkblue,
     colorlinks=true}

%% KEYWORDS
\newenvironment{keywords}{\par\vspace{0.5cm}\noindent{\sffamily\textbf{Keywords:}}}{\vspace{0.25cm}\par\hrule\vspace{0.5cm}\par}

%% ABSTRACT
\renewenvironment{abstract}{\begin{minipage}{\textwidth}\parskip=1.4ex\noindent
\hrule\vspace{0.1cm}\par{\sffamily\textbf{\abstractname}}\newline}
  {\end{minipage}}


\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage[showonlyrefs]{mathtools}
\usepackage[no-weekday]{eukdate}

%% BIBLIOGRAPHY

\makeatletter
\@ifpackageloaded{biblatex}{}{\usepackage[style=authoryear-comp, backend=biber, natbib=true]{biblatex}}
\makeatother
\ExecuteBibliographyOptions{bibencoding=utf8,minnames=1,maxnames=3, maxbibnames=99,dashed=false,terseinits=true,giveninits=true,uniquename=false,uniquelist=false,doi=false, isbn=false,url=true,sortcites=false}

\DeclareFieldFormat{url}{\texttt{\url{#1}}}
\DeclareFieldFormat[article]{pages}{#1}
\DeclareFieldFormat[inproceedings]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[incollection]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[article]{volume}{\mkbibbold{#1}}
\DeclareFieldFormat[article]{number}{\mkbibparens{#1}}
\DeclareFieldFormat[article]{title}{\MakeCapital{#1}}
\DeclareFieldFormat[article]{url}{}
%\DeclareFieldFormat[book]{url}{}
%\DeclareFieldFormat[inbook]{url}{}
%\DeclareFieldFormat[incollection]{url}{}
%\DeclareFieldFormat[inproceedings]{url}{}
\DeclareFieldFormat[inproceedings]{title}{#1}
\DeclareFieldFormat{shorthandwidth}{#1}
%\DeclareFieldFormat{extrayear}{}
% No dot before number of articles
\usepackage{xpatch}
\xpatchbibmacro{volume+number+eid}{\setunit*{\adddot}}{}{}{}
% Remove In: for an article.
\renewbibmacro{in:}{%
  \ifentrytype{article}{}{%
  \printtext{\bibstring{in}\intitlepunct}}}

\AtEveryBibitem{\clearfield{month}}
\AtEveryCitekey{\clearfield{month}}

\makeatletter
\DeclareDelimFormat[cbx@textcite]{nameyeardelim}{\addspace}
\makeatother
\renewcommand*{\finalnamedelim}{%
  %\ifnumgreater{\value{liststop}}{2}{\finalandcomma}{}% there really should be no funny Oxford comma business here
  \addspace\&\space}
  

\wp{no/yr}
\jel{C10,C14,C22}


\blind



\date{\sf\Date~\Month~\Year}
\makeatletter
 \lfoot{\sf\@date}
\makeatother

%% Any special functions or other packages can be loaded here.
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsthm}
\usepackage{amsmath,bm}
\usepackage{paralist}
\usepackage{todonotes}
\usepackage{ctable}

\def\sectionautorefname{Section}
\captionsetup[figure]{font=small}
\captionsetup[table]{font=small}
\def\var{\text{Var}}
\allowdisplaybreaks
\sloppy

%% LINE AND PAGE BREAKING
\clubpenalty = 4500
\widowpenalty = 4500
\brokenpenalty = 4500


\def\yes{$\checkmark$}

\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{5pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}


\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{conjecture}{Conjecture}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[section]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle
\begin{abstract}
To produce good forecasts of time series, we must first solve the
problem of which model to use. Moreover, forecasting accuracy can even
be improved by combining different models. We present an automated
method for combining forecasting models that outperforms any individual
method in a set of the most popular ones in the time series forecasting
literature, achieving second position in the M4 Competition. Our
approach works in two phases. In the first, we use a dataset of time
series to train a meta-model to assign probabilities to the forecasting
methods with the goal of minimizing the average forecasting error. In
the second phase, we forecast new series by assigning probabilities to
methods using our previously trained meta-model, and combining their
individual forecasts using a weighted average. The inputs to this model
are features extracted from the series.
\end{abstract}
\begin{keywords}
FFORMA (Feature-based FORecast-model Averaging), FFORMS (Feature-based
FORecast-model Selection), Time series features, Forecast combination,
XGBoost, M4 Competition
\end{keywords}

\section{Introduction}\label{intro}

There are essentially two general approaches to forecast a time series:
i) use of single model and ii) combination forecast or forecast model
averaging. There is a growing consensus that the combination forecast is
a way of improving forecast accuracy. Empirical work often provides
evidence that the combination of forecasts resulting from several
candidate forecasting models are often superior to their individual
forecasts. Combining forecasts across different models is considered as
a way of reducing the risk of selecting an inappropriate model. However,
the main challenge in forecast combination is the selection of
appropriate set of weights.

\textcite{Bates1969-cu}, was the first to forward the idea of ``the
combination of forecasts''. Since then, it has been extended in various
ways and many approaches have been proposed to derive weights for
forecast combination (\textcite{Aiolfi2011-yz}).

Recently, a few researchers have explored the use of time series
features in selecting the most appropriate forecasting method. However,
the use of time series features to derive weights for forecast
combination has been rarely addressed in the literature. In this paper
we propose a general framework to obtain weights for forecast
combination based on features computed from the time series. We call
this framework FFORMA (Feature-based FORecast Model Averaging). The
proposed FFORMA framework has been used over the course of M4
competition and placed in second in forecast accuracy in both point
forecasts and prediction intervals. This approach has been influenced by
the work of \textcite{fforms} and is related to the previous work by
\textcite{prudencio2004using} in which they use machine learning
techniques to define weights for the linear combination of forecasts
based on five features extracted from the time series.

The rest of the paper is organized as follows: In \autoref{methodology}
we describe the FFORMA framework in a general sense, without specifying
the set of features, forecasting methods and learning model
implementation. This section contains the main contribution: posing the
learning problem in a way that includes the information about the errors
produced by each forecasting method and combines it with the features
extracted from the series. \autoref{M4application} gives the details of
our implementation of FFORMA which achieved second place in the M4
Competition. The required preprocessing steps, the sets of features and
forecast methods, as well as the specific implementation of the
meta-learning model. \emph{The method to produce the prediction
intervals for the M4 competition is also explained in this section.} We
show empirical evidence on the good performance of the approach in
\autoref{results} by quantifiying the difference between our proposal
and a traditional classifier approach using the same features and
underlying learning implementation. \autoref{conclusion} provides some
final remarks and conclusions.

\section{Methodology}\label{methodology}

\subsection{Overview / Intuition}\label{overview-intuition}

The objective of our meta-learning approach is to derive weights to
combine a set of individidual forecasting methods (e.g.~ARIMA,
exponential smoothing) to produce more accurate forecasts. Our framework
requires a dataset of time series, called the \emph{reference set}, from
which we learn to produce the weights for any given time series.

We start from the reference set and a fixed set of forecasting methods,
called the \emph{pool of methods}. Each time series in the reference set
is divided in to training period and test period. The forecasting
methods in the pool is applied to training period of each time series
and forecast error measues are calculated over the test period. Further,
features are computed based on the training period of each time
series.The inputs for training the meta-learning model are: i) features
(e.g.~length, autocorrelation coefficients,etc.) extracted from the
training period of the series and ii) The forecast errors produced by
the forecasting methods.

The model will learn to produce weights for each method in the pool as a
function of the features of the series. Once the meta-learning model is
trained, the inductive step consist of assuming that a new series we
want to forecast comes from a \emph{generating process} similar to the
one that generated the reference set.

One common meta-learning approach is to learn to select the best method
in the pool for each series, the one that produces the least forecasting
error. This approach transforms the problem into a traditional
classification one by setting the individual forecasting methods as the
classes and the best method as the target class for each time series,
enabling the use of existing classification algorithms. However, this
simplification removes relevant information which may be useful in the
metalearning process, such as which methods produce similar errors to
the best one for a series, or which series are more \emph{difficult}
than others. We do not apply this simplification and take into account
the exact error that each method produces in each series, instead of
just considering which one produces the minimum error. This information
is introduced into the model by posing the problem as finding a function
that assigns \emph{probabilities} to each forecasting method for each
series, with the objective of minimizing the expected error that is
produced if the methods were picked at random following these
probabilities.

Our approach can be easily transformed into a classification exercise by
selecting the method corresponds to the largest probability produced by
our model. If we allow a sufficiently rich hypothesis space to fit
freely, we also end up assigning probability one to the methods with the
least amount of error. Our approach can also be seen as a classification
exercise but with \emph{per class weights} (the forecasting errors) that
vary per instance, combined with \emph{per instance weights} that assign
more importance to some series.

\subsection{Algorithmic description}\label{algorithmic-description}

Algorithm \ref{alg:algo-lab} presents the Pseudo code of the proposed
framework. The operation of FFORMA framework can be divided into two
phases: (i) offline phase, in which we train a meta-learner, ii) online
phase, in which we use the pre-trained meta-learner to identify weights
for forecast combination and subsequently generate relevant forecasts.
The offline phase of the FFORMA framework has four main components: i)
reference set, ii) the set of features to be extracted from the time
series, iii) The set of forecasting methods and forecast error measure
and iv)Meta-learning model . In Section 3, we describe these components
in more details with application to M4 competition.

\begin{algorithm}[!ht]
  \caption{The FFORMA framework - Forecast combination based on meta-learning. }
  \label{alg:algo-lab}
  \begin{algorithmic}[1]
    \Statex \textbf{Offline phase - train the learning model}
    \Statex \text{Given:}
    \Statex \hspace{1cm}$O=\{X_1, X_2, \dots,X_n\}:$ the collection of $N$ observed time series, the reference set.
      \Statex \hspace{1cm}$P:$ Set of $K$ forecasting algorithms such as ARIMA, ETS, SNAIVE, etc.
         \Statex \hspace{1cm}$F:$ the set of functions to calculate time series features.
         \Statex \hspace{1cm}$E:$ A forecasting error measure such as Mean Squared Error.
     \Statex \text{Output:}
      \Statex \hspace{1cm}\text{FFORMA meta-learner: A function from the extracted features }
      \Statex \text{\hspace{1cm} to a set of $K$ probabilities, one for each method in $P$.}
    \Statex \textit{Prepare the meta-data}
    \Statex For $j=1$ to $N$:
            \State Split $X_j$ into a training period and test period.
            \State Calculate the set of features for the training period by applying $F$.
            \State Fit the models in $P$ to the training period.
            \State Calculate forecasts for the test period from each model.
            \State Calculate forecast error measure $E$ over the test period for all models in $P$.
            \State Meta-data: input features $x_j$ (step 2), output errors: $e_j$ (step 5).
     \Statex
    \Statex \textit{Train the meta-learner}
            \State Train a learning model based on the meta-data and errors, by minimizing:
            $$ argmin_f \sum_{j=1}^N \sum_{k=1}^K f(x_j)_k e_{jk}  $$
            \State {meta-learner}.
    \Statex
     \Statex \textbf{Online phase - forecast a new time series}
    \Statex \text{Given:}
    \Statex \hspace{1cm}\text{FFORMA classifier from step 8} .
     \Statex \text{Output:}
      \Statex \hspace{1cm}\text{Forecast for the new time series $X_{new}$}.
  \State For $X_{new}$ calculate features $x_{new}$ by applying $F$.
  \State From the features, use the meta-learner to produce $w$ the vector of probabilities. 
  \State Compute the individual forecasts of the methods in $P$ for $X_{new}$
  \State Compute the final forecast by weighted average using $w$ and the individual forecasts.
   \end{algorithmic}

\end{algorithm}

\section{Implementation and Application to M4
Competition}\label{M4application}

\subsection{Reference set}\label{reference-set}

For the M4 competition, we used the whole M4 data set as the reference
set. The M4 competition database consists of 100,000 real-world time
series of yearly, quarterly, monthly, weekly, daily and hourly data. To
divide the series into training and testing period, we used the forecast
horizon specified in the database as the size of the temporal holdout.
When the training period resulted too short (less than 2 periods), the
size of the temporal holdout was reduced. Some series were removed
because they were constant after removing the holdout part.

\subsection{Time series features}\label{time-series-features}

We use a set of 42 features. \autoref{feature} provides a brief
description of features used in this experiment. The functions to
calculate these features are implemented in \texttt{tsfeatures} R
package by \textcite{tsfeatures}. Most of the features have been
previously used by \textcite{hyndman2015large} and \textcite{fforms}.
The last five heterogeneity features are calculated as below. First, the
time series is pre-whitened using an AR model and obtain a new series
\(z\). Then, a GARCH(1, 1) model is fitted to the pre-whitened series
and obtain the residual(\(r\)) series. Subsequently, the features 39-42
are computed. A detailed description of other features are provided in
\textcite{fforms}. The features corresponds to seasonal time series only
are set to zero for the case of non-seasonal time series. Note that, we
have not used any of the domain-specific features such as macro, micro,
finance, etc., even though these information are available in the M4
data set.

\begin{table}[!htp]
\centering\footnotesize\tabcolsep=0.12cm
\caption{Features used in FFORMA framework.}
\label{feature}
\begin{tabular}{llp{8,8cm}cc}
\toprule
\multicolumn{2}{c}{Feature} & Description & Non-seasonal & Seasonal\\
\midrule
1  & T              & length of time series                                                                   & \yes  & \yes \\
2  & trend          & strength of trend                                                                       & \yes  & \yes\\
3  & seasonality   & strength of seasonality                                                                 & -     & \yes \\
4  & linearity      & linearity                                                                               & \yes  & \yes \\
5  & curvature      & curvature                                                                               & \yes  & \yes \\
6  & spikiness      & spikiness                                                                               & \yes  & \yes \\
7  & e\_acf1        & first ACF value of remainder series                                                     & \yes  & \yes \\
8  & e\_acf10        & sum of squares of first 10 ACF values of remainder series                                                    & \yes  & \yes \\
9  & stability      & stability                                                                               & \yes  & \yes \\
10  & lumpiness      & lumpiness                                                                               & \yes  & \yes \\
11 & entropy        & spectral entropy                                                                        & \yes  & \yes \\
12 & hurst          & Hurst exponent                                                                          & \yes  & \yes \\
13 & nonlinearity   & nonlinearity                                                                            & \yes\ & \yes \\
13 & alpha          & ETS(A,A,N) $\hat\alpha$                                                                 & \yes  & \yes \\
14 & beta           & ETS(A,A,N) $\hat\beta$                                                                  & \yes  & \yes\\
15 & hwalpha        & ETS(A,A,A) $\hat\alpha$                                                                 & -     & \yes \\
16 & hwbeta         & ETS(A,A,A) $\hat\beta$                                                                  & -     & \yes \\
17 & hwgamma        & ETS(A,A,A) $\hat\gamma$                                                                 & -     & \yes \\
18 & ur\_pp         & test statistic based on Phillips-Perron test                                            & \yes  & \yes \\
19 & ur\_kpss       & test statistic based on KPSS test                                                       & \yes  & \yes \\
20 & y\_acf1        & first ACF value of the original series                                                  & \yes  & \yes \\
21 & diff1y\_acf1   & first ACF value of the differenced series                                               & \yes  & \yes \\
22 & diff2y\_acf1   & first ACF value of the twice-differenced series                                         & \yes  & \yes \\
23 & y\_acf10        & sum of squares of first 10 ACF values of original series                                 & \yes  & \yes \\
24 & diff1y\_acf10   & sum of squares of first 10 ACF values of differenced series                              & \yes  & \yes \\
25 & diff2y\_acf10   & sum of squares of first 10 ACF values of twice-differenced series                        & \yes  & \yes \\
26 & seas\_acf1     & autocorrelation coefficient at first seasonal lag                                       & -     & \yes \\
27 & sediff\_acf1   & first ACF value of seasonally-differenced series                                        & -     & \yes\\
28 & y\_pacf5       & sum of squares of first 5 PACF values of original series                                & \yes  & \yes \\
29 & diff1y\_pacf5  & sum of squares of first 5 PACF values of differenced series                             & \yes  & \yes \\
30 & diff2y\_pacf5  & sum of squares of first 5 PACF values of twice-differenced series                       & \yes  & \yes \\
31 & seas\_pacf  & partial autocorrelation coefficient at first seasonal lag                       & \yes  & \yes \\
32 & crossing\_point  & number of times the time series crosses the median                     & \yes  & \yes \\
33 & flat\_spots  & number of flat spots, calculated by discretizing the series into 10 equal sized intervals and counting the maximum run length within any single interval                       & \yes  & \yes \\
34 & nperiods  & number of seasonal periods in the series & -  & \yes \\
35 & seasonal\_period  & length of seasonal period                       & -  & \yes \\
36 & peak  & strength of peak                      & \yes  & \yes \\
37 & trough  & strength of trough                      & \yes  & \yes \\
38 & ARCH.LM  & ARCH LM statistic                      & \yes  & \yes \\
39 & arch\_acf  &    sum of squares of the first 12 autocorrelations of $z^2$               & \yes  & \yes \\
40 & garch\_acf  &  sum of squares of the first 12 autocorrelations of $r^2$                 & \yes  & \yes \\
41 & arch\_r2  &     $R^2$ value of an AR model applied to $Z^2$              & \yes  & \yes \\
42 & garch\_r2  &   $R^2$ value of an AR model applied to $e^2$                 & \yes  & \yes \\
\bottomrule
 \end{tabular}
\end{table}

\subsection{Forecasting methods}\label{forecasting-methods}

We considered nine forecasting algorithms implemented in the
\texttt{forecast}\autocite{forecast} package in R. They are (the
specific R calls for fitting the models are given):i) automated ARIMA
algorithm (\texttt{auto.arima}), ii) automated ETS algorithm
(\texttt{ets}), iii) feed-forward neural network with a single hidden
layer is fitted to the lags. The number of lags is automatically
selected (\texttt{nnetar}), iv) random walk with drift (\texttt{rwf}
with drift=TRUE), v) TBATS model (\texttt{tbats}), vi) Theta method
forecast (\texttt{thetaf}), vii) naive forecasts (\texttt{naive}), viii)
STLM-AR Seasonal and Trend decomposition using Loess with AR modeling of
the seasonally adjusted series(\texttt{stlm} with model function
\texttt{ar}) and ix) seasonal naive forecasts (\texttt{snaive}). In all
cases, the default setting is used. Further, in the case of an error
when fitting the series (e.g.~a series is constant), the SNAIVE forecast
method is used instead.

\todo{Forecast-error measure calculation: OWI}

\subsection{Metal-learning model}\label{metal-learning-model}

We choose the gradient tree boosting model of \textbf{xgboost}
(\textcite{chen2016xgboost}) as the underlying implementation of the
learning model due to following reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ability to customize the model to fit our objective function
\item
  Computational efficiency: The size of the M4 data set requires a
  efficient model
\item
  Good performance in structure based problems
\end{enumerate}

\subsubsection{The xgboost objective
function}\label{the-xgboost-objective-function}

The vanilla xgboost algorithm produces numeric values from the features,
one for each forecasting method in our pool. We apply the softmax
transform to these values prior to computing the the objective function
shown in Algorithm \ref{alg:algo-lab} step 7, implemented as a custom
objective function.

xgboost fits the model by gradient boosting, for which requires a
gradient and hessian of the objective function. In our case, the
gradient is the direct gradient of the objective, but the \emph{correct}
hessian is prone to numerical problems that need to be fixed for xgboost
to converge. This is a relative common problem and one simple fix is to
use an upper bound of the hessian by clamping small values to a larger
one. In our case, we compute an alternate upper bound of the hessian by
removing some terms from the \emph{correct} hessian. Although both
alternatives converge, our approach works faster, requiring less
boosting steps to converge. This not only increases the computational
efficiency, it also generalizes better due to a less complex set of
trees produced in the final solution.

The functions involved in computing the objective are the following:

\begin{itemize}
\tightlist
\item
  \(y(x)\) is the output of the xgboost algorithm
\item
  \(p_j = \frac{e^{y(x)_j}}{ \sum_k e^{y(x)_k}}\) is the transformation
  to probabilities by applying the softmax transform.
\item
  \(L = \sum p_j e_j\) is the loss of the function \(p\).
\item
  \(G_j = \frac{\partial{L}}{\partial{p_j}} = p_j(e_j -\sum_k e_kp_k)\)
  \[ H_j = \frac{\partial{G_j}}{\partial{p_j}} \approx \hat{H}_j = p_j(e_j(1-p_j) - G_j)  \]
\end{itemize}

\subsubsection{Hyperparameters}\label{hyperparameters}

The results of xgboost are particularly dependent on its
hyper-parameters such as learning rate, number of boosting steps,
maximum complexity allowed for the trees or subsampling sizes. In our
case we limit the hyper-parameter search space based on some initial
results and rules of thumb and explore it using Bayesian optimization,
(implemented in the \texttt{rBayesianOptimization} R package
\autocite{rBayesianOptimization}) measuring performance on a 10\%
holdout version of the reference set. We picked the most simple
hyper-parameter set from the top solutions of the exploration.

\subsection{Prediction Intervals}\label{prediction-intervals}

The M4 Competition also featured a sub competition over the accuracy of
prediction intervals. For this part, we used a different approach. In
order to compute the intervals we used the point forecast produced by
our meta-learner as the center of the interval and computed the 95\%
bounds of the interval by a linear combination of the bounds of three
forecasting methods: Thetaf, SNAIVE and NAIVE methods. The coefficients
for the linear combination were calculated in a data driven way also
over the M4 database. The procedure is as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For the training period each series in the data set:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Compute the point forecast of the meta-learning.
  \item
    Compute the 95\% \emph{predicion radius} for the thetaf, snaive and
    naive, this is the difference between the 95\% upper bound and the
    point forecast for each horizon.
  \end{enumerate}
\item
  For each forecasting horizon required in the data set:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Find the coefficients that minimize the MSIS error of the interval
    with the meta-learning point forecast as center and a linear
    combination of the radiuses of thetaf, snaive and naive as radius.
    The minimization is done by gradient descent.
  \end{enumerate}
\end{enumerate}

This procedure produces at set of three coefficients for each prediction
horizon in the M4 dataset and these coefficients will be the same
independently of the series we want to forecast. \emph{These
coefficients are not restricted to be probabilities, the optimization is
unrestricted.} In order to prevent overfitting for the center of the
intervals, the M4 dataset was divided in two halves, and our
metalearning approach was trained in one half and applied to the other.

\section{Results}\label{results}

We quantify the improvement in accuracy produced by our approach,
compared to a classification implementation using the same underlying
implementation, xgboost.

\section{Discussion and Conclusions}\label{conclusion}

\printbibliography[title=References]

\end{document}
