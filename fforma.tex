\documentclass[11pt,a4paper,]{article}
\usepackage{lmodern}

\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdftitle={FFORMA: Feature-based FORecast-model Averaging},
            pdfkeywords={FFORMS (Feature-based FORecast-model Selection), Time series features,
Forecast combination, XGBoost, M4 Competition, Meta-learning},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{geometry}
\geometry{a4paper, text={16cm,24cm}}
\usepackage[style=authoryear-comp,]{biblatex}
\addbibresource{references.bib}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{long table}}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


\title{FFORMA: Feature-based FORecast-model Averaging}

%% MONASH STUFF

%% CAPTIONS
\RequirePackage{caption}
\DeclareCaptionStyle{italic}[justification=centering]
 {labelfont={bf},textfont={it},labelsep=colon}
\captionsetup[figure]{style=italic,format=hang,singlelinecheck=true}
\captionsetup[table]{style=italic,format=hang,singlelinecheck=true}

%% FONT
\RequirePackage{bera}
\RequirePackage{mathpazo}

%% HEADERS AND FOOTERS
\RequirePackage{fancyhdr}
\pagestyle{fancy}
\rfoot{\Large\sffamily\raisebox{-0.1cm}{\textbf{\thepage}}}
\makeatletter
\lhead{\textsf{\expandafter{\@title}}}
\makeatother
\rhead{}
\cfoot{}
\setlength{\headheight}{15pt}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\fancypagestyle{plain}{%
\fancyhf{} % clear all header and footer fields
\fancyfoot[C]{\sffamily\thepage} % except the center
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}}

%% MATHS
\RequirePackage{bm,amsmath}
\allowdisplaybreaks

%% GRAPHICS
\RequirePackage{graphicx}
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.8}

%\RequirePackage[section]{placeins}

%% SECTION TITLES
\RequirePackage[compact,sf,bf]{titlesec}
\titleformat{\section}[block]
  {\fontsize{15}{17}\bfseries\sffamily}
  {\thesection}
  {0.4em}{}
\titleformat{\subsection}[block]
  {\fontsize{12}{14}\bfseries\sffamily}
  {\thesubsection}
  {0.4em}{}
\titlespacing{\section}{0pt}{*5}{*1}
\titlespacing{\section}{0pt}{*2}{*0.2}


%% TITLE PAGE
\def\Date{\number\day}
\def\Month{\ifcase\month\or
 January\or February\or March\or April\or May\or June\or
 July\or August\or September\or October\or November\or December\fi}
\def\Year{\number\year}

\makeatletter
\def\wp#1{\gdef\@wp{#1}}\def\@wp{??/??}
\def\jel#1{\gdef\@jel{#1}}\def\@jel{??}
\def\showjel{{\large\textsf{\textbf{JEL classification:}}~\@jel}}
\def\nojel{\def\showjel{}}
\def\addresses#1{\gdef\@addresses{#1}}\def\@addresses{??}
\def\cover{{\sffamily\setcounter{page}{0}
        \thispagestyle{empty}%
        \vspace*{-2cm}
        \centerline{\raisebox{-1.8cm}{\includegraphics[width=5cm]{MBSportrait}}\hspace*{9cm} ISSN 1440-771X}\vspace{0.99cm}
        \begin{center}\Large
        Department of Econometrics and Business Statistics\\[.5cm]
        \scriptsize http://business.monash.edu/econometrics-and-business-statistics/research/publications
        \end{center}\vspace{2cm}
        \begin{center}
        \fbox{\parbox{14cm}{\begin{onehalfspace}\centering\Huge\vspace*{0.3cm}
                \textsf{\textbf{\expandafter{\@title}}}\vspace{1cm}\par
                \LARGE\@author\end{onehalfspace}
        }}
        \end{center}
        \vfill
                \begin{center}\Large
                \Month~\Year\\[1cm]
                Working Paper \@wp
        \end{center}}}
\def\pageone{{\sffamily
        \newpage%\blankpage
        \thispagestyle{empty}
        \vbox to 23cm{
        \raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}
        \vspace{2cm}\par
        \hspace{1cm}\parbox{14cm}{\sffamily\large\@addresses}\vspace{1cm}\vfill
        \hspace{1cm}{\large\Date~\Month~\Year}\\[1cm]
        \hspace{1cm}\showjel\vss}}}
\def\blindtitle{{\sffamily
     \thispagestyle{plain}\raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}\vspace{1cm}\par
        }}
\def\titlepage{{\cover\newpage\setstretch{1}\pageone\newpage\blindtitle}}

\def\blind{\def\titlepage{{\blindtitle}}\let\maketitle\blindtitle}
\def\titlepageonly{\def\titlepage{{\pageone\end{document}}}}
\def\nocover{\def\titlepage{{\newpage\setstretch{1}\pageone\newpage\blindtitle}}\let\maketitle\titlepage}
\let\maketitle\titlepage
\makeatother

%% SPACING
\RequirePackage{setspace}
\spacing{1.5}

%% LINE AND PAGE BREAKING
\sloppy
\clubpenalty = 10000
\widowpenalty = 10000
\brokenpenalty = 10000
\RequirePackage{microtype}

%% PARAGRAPH BREAKS
\setlength{\parskip}{1.4ex}
\setlength{\parindent}{0em}

%% HYPERLINKS
\RequirePackage{xcolor} % Needed for links
\definecolor{darkblue}{rgb}{0,0,.6}
\RequirePackage{url}

\makeatletter
\@ifpackageloaded{hyperref}{}{\RequirePackage{hyperref}}
\makeatother
\hypersetup{
     citecolor=0 0 0,
     breaklinks=true,
     bookmarksopen=true,
     bookmarksnumbered=true,
     linkcolor=darkblue,
     urlcolor=blue,
     citecolor=darkblue,
     colorlinks=true}

%% KEYWORDS
\newenvironment{keywords}{\par\vspace{0.5cm}\noindent{\sffamily\textbf{Keywords:}}}{\vspace{0.25cm}\par\hrule\vspace{0.5cm}\par}

%% ABSTRACT
\renewenvironment{abstract}{\begin{minipage}{\textwidth}\parskip=1.4ex\noindent
\hrule\vspace{0.1cm}\par{\sffamily\textbf{\abstractname}}\newline}
  {\end{minipage}}


\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage[showonlyrefs]{mathtools}
\usepackage[no-weekday]{eukdate}

%% BIBLIOGRAPHY

\makeatletter
\@ifpackageloaded{biblatex}{}{\usepackage[style=authoryear-comp, backend=biber, natbib=true]{biblatex}}
\makeatother
\ExecuteBibliographyOptions{bibencoding=utf8,minnames=1,maxnames=3, maxbibnames=99,dashed=false,terseinits=true,giveninits=true,uniquename=false,uniquelist=false,doi=false, isbn=false,url=true,sortcites=false}

\DeclareFieldFormat{url}{\texttt{\url{#1}}}
\DeclareFieldFormat[article]{pages}{#1}
\DeclareFieldFormat[inproceedings]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[incollection]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[article]{volume}{\mkbibbold{#1}}
\DeclareFieldFormat[article]{number}{\mkbibparens{#1}}
\DeclareFieldFormat[article]{title}{\MakeCapital{#1}}
\DeclareFieldFormat[article]{url}{}
%\DeclareFieldFormat[book]{url}{}
%\DeclareFieldFormat[inbook]{url}{}
%\DeclareFieldFormat[incollection]{url}{}
%\DeclareFieldFormat[inproceedings]{url}{}
\DeclareFieldFormat[inproceedings]{title}{#1}
\DeclareFieldFormat{shorthandwidth}{#1}
%\DeclareFieldFormat{extrayear}{}
% No dot before number of articles
\usepackage{xpatch}
\xpatchbibmacro{volume+number+eid}{\setunit*{\adddot}}{}{}{}
% Remove In: for an article.
\renewbibmacro{in:}{%
  \ifentrytype{article}{}{%
  \printtext{\bibstring{in}\intitlepunct}}}

\AtEveryBibitem{\clearfield{month}}
\AtEveryCitekey{\clearfield{month}}

\makeatletter
\DeclareDelimFormat[cbx@textcite]{nameyeardelim}{\addspace}
\makeatother
\renewcommand*{\finalnamedelim}{%
  %\ifnumgreater{\value{liststop}}{2}{\finalandcomma}{}% there really should be no funny Oxford comma business here
  \addspace\&\space}
  

\wp{no/yr}
\jel{C10,C14,C22}


\blind



\date{\sf\Date~\Month~\Year}
\makeatletter
 \lfoot{\sf\@date}
\makeatother

%% Any special functions or other packages can be loaded here.
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsthm}
\usepackage{amsmath,bm}
\usepackage{paralist}
\usepackage{todonotes}
\usepackage{ctable}

\def\sectionautorefname{Section}
\captionsetup[figure]{font=small}
\captionsetup[table]{font=small}
\def\var{\text{Var}}
\allowdisplaybreaks
\sloppy

%% LINE AND PAGE BREAKING
\clubpenalty = 4500
\widowpenalty = 4500
\brokenpenalty = 4500


\def\yes{$\checkmark$}

\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{5pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}


\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[section]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle
\begin{abstract}
To produce good forecasts of time series, we must first solve the
problem of which model to use. Moreover, forecasting accuracy can even
be improved by combining different models. We present an automated
method for combining forecasting models that outperforms any individual
method in a set of the most popular ones in the time series forecasting
literature, achieving second position in the M4 Competition. Our
approach works in two phases. In the first, we use a dataset of time
series to train a meta-model to assign probabilities to the forecasting
methods with the goal of minimizing the average forecasting error. In
the second phase, we forecast new series by assigning probabilities to
methods using our previously trained meta-model, and then combine their
individual forecasts using a weighted average with these probabilities.
The inputs to the meta-model are features extracted from the series.
\end{abstract}
\begin{keywords}
FFORMS (Feature-based FORecast-model Selection), Time series features,
Forecast combination, XGBoost, M4 Competition, Meta-learning
\end{keywords}

\section{Introduction}\label{intro}

There are essentially two general approaches for forecasting a time
series: (i) generating forecasts from a single model, and (ii) combining
forecasts from many models or forecast model averaging. There has been a
vast literature on the latter motivated by the seminal work of
\textcite{Bates1969-cu} and followed by a plethora of empirical
applications showing that combination forecasts are often superior to
their individual counterparts (see among others,
\textcite{clemen1989combining}, \todo{maybe add more here}). Combining
forecasts is considered a successful way of hedging against the risk of
selecting a misspecified model. Of course the main challenge is in
selecting an appropriate set of weights with many contributions (see for
example Timmermann 2006, \textcite{Aiolfi2011-yz}) attempting to
overcome the so called ``forecast combination puzzle''.

Recently, \textcite{fforms} have explored the idea of using time series
features combined with meta-learning for forecast-model selection; a
framework labelled as FFORMS (Feature-based FORecast Model Selection).
\textcolor{red}{ maybe cite here some papers on feature-based approaches, }\textcite{prudencio2004using}
\textcolor{red}{is a good one and } \textcite{LEMKE20102006}
\textcolor{red}{is the most cited one.} In this paper we build on this
framework posing the learning problem in a way that includes information
about the forecast errors produced by the set of forecasting methods
considered and combines these with features extracted from the time
series in order to derive a set of weights for forecast combinations. We
label this framework FFORMA (Feature-based FORecast Model Averaging).
The proposed FFORMA framework has been used over the course of the M4
competition and resulted in the second most accurate point forecasts and
prediction intervals amongst all competitors.

The rest of the paper is organized as follows: In \autoref{methodology}
we describe the FFORMA framework in a very general sense.
\autoref{M4application} gives the details of our implementation of
FFORMA in the M4 Competition for generating both point and interval
forecasts. This includes the required preprocessing steps, the set of
features and forecast methods, as well as the specific implementation of
the meta-learning model. We show empirical evidence on the performance
of the approach in \autoref{results} by quantifying the difference
between our proposed learning model and a traditional classifier
approach. \autoref{conclusion} provides some final remarks and
conclusions.

\section{Methodology}\label{methodology}

\subsection{Overview / Intuition of
FFORMA}\label{overview-intuition-of-fforma}

The objective of our meta-learning approach is to derive a set of
weights to combine forecasts generated from a \emph{pool of methods}
(e.g., na?ve, ETS, ARIMA, etc.). The FFORMA framework requires a set of
time series we refer to as the \emph{reference set}. From these the
algorithm learns in order to produce forecast combination weights for
any given time series.

Each time series in the reference set is divided into a training period
and a test period. From the training period a set of \emph{time series
features} are calculated (e.g., length of time series, strength of
trend, etc.). These form the inputs for training the meta-learning
model. Moreover, each method in the pool is fitted to the training
period and forecasts are generated over the test period. From these a
\emph{forecast error measure} is computed for each method. These form
the
\textcolor{red}{ supervised part for the training of the meta-learning model.}
(\textcolor{blue}{(Not sure if calling these labels may be confusing)labels ?)}
\}

The meta-learning model learns to produce weights for each method in the
pool as a function of the features of the series. Once the model is
trained, the inductive step consist of assuming that a new series for
which we require forecasts comes from a \emph{generating process}
similar to the one that generated the reference set.

A common meta-learning approach is to learn to select the best method in
the pool of methods for each series, i.e., the one that produces the
smallest forecast error. This approach transforms the problem into a
traditional classification problem by setting the individual forecasting
methods as the classes and the best method as the target class for each
time series, enabling the use of existing classification algorithms.
This is what was implemented by \textcite{fforms} and formed the basis
of the FFORMS approaph.

However, this simplification removes relevant information which may be
useful in the meta-learning process, such as which methods produce
similar errors to the most accurate one, or which series
\textcolor{red}{ have larger impact in the total error}
\textcolor{blue}{( are more *difficult* than others)}
\todo{Not sure what difficult means here and how it is used - difficult to forecast? and then?}.
In this paper we advance the FFORMS framework by not applying this
simplification and taking into account the exact error that each method
produces in each series, instead of just considering the one that
produces the minimum error.

This information is introduced into the model by posing the problem as
finding a function that assigns \emph{probabilities} to each forecasting
method for each series, with the objective of minimizing the expected
error that is produced if the methods were picked at random following
these probabilities.

\textcolor{red}{This approach approach is more general than classification, and can be thought of as classification  with *per class weights* (the forecasting errors) that vary per instance, combined with *per instance weights* that assign more importance to some series.}

\textcolor{blue}{
Our approach can be easily transformed into a classification exercise by selecting the method corresponding to the largest probability produced by our model. If we allow a sufficiently rich hypothesis space to fit freely, we also end up assigning probability one to the method with the smallest error. Hence our approach can also be seen as a classification exercise, but with *per class weights* (the forecasting errors) that vary per instance, combined with *per instance weights* that assign more importance to some series.}\todo{I think a lot of this - even all of it - can go if we need more space.}

\subsection{Algorithmic description}\label{algorithmic-description}

The operation of the FFORMA framework comprises two phases: (i) offline
phase, in which we train a meta-learner and (ii) online phase, in which
we use the pre-trained meta-learner to identify forecast combination
weights. Algorithm \ref{alg:algo-lab} presents in detail the Pseudo code
of the proposed framework.

\begin{algorithm}[!ht]
  \caption{The FFORMA framework - Forecast combination based on meta-learning. }
  \label{alg:algo-lab}
  \begin{algorithmic}[1]
    \Statex \textbf{Offline phase - train the learning model}
    \Statex \text{Given:}
    \Statex \hspace{1cm}$\{X_1, X_2, \dots,X_N\}:$ $N$ observed time series forming the reference set.
      \Statex \hspace{1cm}$P:$ the pool of $K$ forecasting methods, e.g., na?ve, ETS, ARIMA, etc.
         \Statex \hspace{1cm}$F:$ the set of functions for calculating time series features.
         \Statex \hspace{1cm}$E:$ A forecast error measure, e.g., MASE, sMAPE, etc.
           \Statex \hspace{1cm}$L:$ Loss function.
           \Statex \hspace{1cm}$M:$ number of iterations in the xgboost algorithm.
     \Statex \text{Output:}
      \Statex \hspace{1cm}\text{FFORMA meta-learner: A function from the extracted features }
      \Statex \text{\hspace{1cm} to a set of $K$ probabilities, one for each method in $P$.}
    \Statex \textit{Prepare the meta-data}
    \Statex For $j=1$ to $N$:
            \State Split $X_j$ into a training period and test period.
            \State Calculate the set of features $\bm{f}_j$ over the training period by applying $F$.
            \State Fit each method in $P$ over the training period and generate forecasts over the test period.
            \State Calcuate forecast error measure  $e_{jk}$ over the test period for each method $k=1,\ldots, K$ in $P$.
 \textcolor{green}{\State Meta-data: input features $\bm{f}_j$ (step 2), output errors: $\bm{e}_{jk}$ (step 5).
     \Statex
    \Statex \textit{Train the meta-learner, $y$}
            \State Train a learning model based on the meta-data and errors, by minimizing:
            $$ argmin_y \sum_{j=1}^N \sum_{k=1}^K y(x_j)_k e_{jk}  $$
            \State {Meta-learner, $y$}.}
    \Statex
     \Statex \textbf{Online phase - forecast a new time series}
    \Statex \text{Given:}
    \Statex \hspace{1cm}\text{FFORMA classifier from step 7} .
     \Statex \text{Output:}
      \Statex \hspace{1cm}\text{Forecast for the new time series $X_{new}$}.
  \State For $X_{new}$ calculate features $\bm{f}_{new}$ by applying $F$.
  \State From the features, use the meta-learner to produce $y(\bm{f}_{new})=\bf{w}$ the vector of probabilities. 
  \State Compute the individual forecasts of the methods in $P$ for $X_{new}$
  \State Combine individual forecasts using $\bm{w}$ to generate final forecasts for {$X_{new}$}.
   \end{algorithmic}

\end{algorithm}

\clearpage

\section{Implementation and Application to M4
Competition}\label{M4application}

\subsection{Reference set}\label{reference-set}

The M4 dataset includes 100,000 real-world time series of yearly,
quarterly, monthly, weekly, daily and hourly data. All 100,000 series
form the reference set. Each series is split into training period and a
test period. The length of the test period for each time series was set
to be equal to the forecast horizon set by the competition. When the
resulting training period was too short (fewer than 2
\textcolor{red}{periods}\textcolor{blue}{(observations)}), the size of
the test period was reduced. The series that were simply a constant over
the training period were eliminated from any further analysis.

\subsection{Time series features}\label{time-series-features}

\autoref{feature} provides a brief description of the 42 features used
in this experiment, \textcolor{red}{$F$ in Algorithm}
\ref{alg:algo-lab}. The functions to calculate these are implemented in
the \texttt{tsfeatures} R package by \textcite{tsfeatures}. Most of the
features have been previously used by \textcite{hyndman2015large} and
\textcite{fforms}. The last five heterogeneity features are calculated
as follows. First, the time series is pre-whitened using an AR model
resulting to a new series \(z\). A GARCH(1, 1) model is then fitted to
the pre-whitened series to obtain the residual series, \(r\).
Subsequently, features 39-42 are computed. The ARCH.LM statistic is
calculated based on Lagrange Multiplier test of
\textcite{engle1982autoregressive} for autoregressive conditional
heteroscedasticity (ARCH). A detailed description of the other features
is provided in \textcite{fforms}. The features corresponding to seasonal
time series only, are set to zero for the case of non-seasonal time
series. Note that, for the sake of generality, we have not used any of
the domain-specific features such as macro, micro, finance, etc., even
though this information was available in the M4 data
set.\todo{Should we add here in an effort to be as general as possible.}

\begin{table}[!htp]
\centering\footnotesize\tabcolsep=0.12cm
\caption{Features used in FFORMA framework.}
\label{feature}
\begin{tabular}{llp{8,8cm}cc}
\toprule
\multicolumn{2}{c}{Feature} & Description & Non-seasonal & Seasonal\\
\midrule
1  & T              & length of time series                                                                   & \yes  & \yes \\
2  & trend          & strength of trend                                                                       & \yes  & \yes\\
3  & seasonality    & strength of seasonality                                                                 & -     & \yes \\
4  & linearity      & linearity                                                                               & \yes  & \yes \\
5  & curvature      & curvature                                                                               & \yes  & \yes \\
6  & spikiness      & spikiness                                                                               & \yes  & \yes \\
7  & e\_acf1        & first ACF value of remainder series                                                     & \yes  & \yes \\
8  & e\_acf10       & sum of squares of first 10 ACF values of remainder series                                                    & \yes  & \yes \\
9  & stability      & stability                                                                               & \yes  & \yes \\
10  & lumpiness      & lumpiness                                                                               & \yes  & \yes \\
11 & entropy        & spectral entropy                                                                        & \yes  & \yes \\
12 & hurst          & Hurst exponent                                                                          & \yes  & \yes \\
13 & nonlinearity   & nonlinearity                                                                            & \yes\ & \yes \\
13 & alpha          & ETS(A,A,N) $\hat\alpha$                                                                 & \yes  & \yes \\
14 & beta           & ETS(A,A,N) $\hat\beta$                                                                  & \yes  & \yes\\
15 & hwalpha        & ETS(A,A,A) $\hat\alpha$                                                                 & -     & \yes \\
16 & hwbeta         & ETS(A,A,A) $\hat\beta$                                                                  & -     & \yes \\
17 & hwgamma        & ETS(A,A,A) $\hat\gamma$                                                                 & -     & \yes \\
18 & ur\_pp         & test statistic based on Phillips-Perron test                                            & \yes  & \yes \\
19 & ur\_kpss       & test statistic based on KPSS test                                                       & \yes  & \yes \\
20 & y\_acf1        & first ACF value of the original series                                                  & \yes  & \yes \\
21 & diff1y\_acf1   & first ACF value of the differenced series                                               & \yes  & \yes \\
22 & diff2y\_acf1   & first ACF value of the twice-differenced series                                         & \yes  & \yes \\
23 & y\_acf10        & sum of squares of first 10 ACF values of original series                                 & \yes  & \yes \\
24 & diff1y\_acf10   & sum of squares of first 10 ACF values of differenced series                              & \yes  & \yes \\
25 & diff2y\_acf10   & sum of squares of first 10 ACF values of twice-differenced series                        & \yes  & \yes \\
26 & seas\_acf1     & autocorrelation coefficient at first seasonal lag                                       & -     & \yes \\
27 & sediff\_acf1   & first ACF value of seasonally-differenced series                                        & -     & \yes\\
28 & y\_pacf5       & sum of squares of first 5 PACF values of original series                                & \yes  & \yes \\
29 & diff1y\_pacf5  & sum of squares of first 5 PACF values of differenced series                             & \yes  & \yes \\
30 & diff2y\_pacf5  & sum of squares of first 5 PACF values of twice-differenced series                       & \yes  & \yes \\
31 & seas\_pacf  & partial autocorrelation coefficient at first seasonal lag                       & \yes  & \yes \\
32 & crossing\_point  & number of times the time series crosses the median                     & \yes  & \yes \\
33 & flat\_spots  & number of flat spots, calculated by discretizing the series into 10 equal sized intervals and counting the maximum run length within any single interval                       & \yes  & \yes \\
34 & nperiods  & number of seasonal periods in the series & -  & \yes \\
35 & seasonal\_period  & length of seasonal period                       & -  & \yes \\
36 & peak  & strength of peak                      & \yes  & \yes \\
37 & trough  & strength of trough                      & \yes  & \yes \\
38 & ARCH.LM  & ARCH LM statistic                      & \yes  & \yes \\
39 & arch\_acf  &    sum of squares of the first 12 autocorrelations of $z^2$               & \yes  & \yes \\
40 & garch\_acf  &  sum of squares of the first 12 autocorrelations of $r^2$                 & \yes  & \yes \\
41 & arch\_r2  &     $R^2$ value of an AR model applied to $z^2$              & \yes  & \yes \\
42 & garch\_r2  &   $R^2$ value of an AR model applied to \textcolor{blue}{$r^2$}               & \yes  & \yes \\
\bottomrule
 \end{tabular}
\end{table}

\subsection{Pool of forecasting
methods}\label{pool-of-forecasting-methods}

\textcolor{red}{For the pool of methods, $P$ in Algorithm }
\ref{alg:algo-lab}, we consider nine methods implemented in the
\texttt{forecast} \autocite{forecast} package R package. They are (the
specific R calls for fitting the models are given):

\begin{enumerate}
\def\labelenumi{\roman{enumi})}
\item
  automated ARIMA algorithm (\texttt{auto.arima}),
\item
  automated ETS algorithm (\texttt{ets}),
\item
  feed-forward neural network with a single hidden layer fitted to the
  lags. The number of lags is automatically selected (\texttt{nnetar}),
\item
  random walk with drift (\texttt{rwf} with drift=TRUE),
\item
  TBATS model (\texttt{tbats}),
\item
  Theta method (\texttt{thetaf}),
\item
  naive (\texttt{naive}),
\item
  STLM-AR Seasonal and Trend decomposition using Loess with AR modeling
  of the seasonally adjusted series (\texttt{stlm} with model function
  \texttt{ar}) and
\item
  seasonal naive (\texttt{snaive}).
\end{enumerate}

In all cases, the default setting is used. Further, in the case of an
error when fitting the series (e.g.~a series is constant), the SNAIVE
forecast method is used instead.

\subsection{Forecast-error measure}\label{forecast-error-measure}

\textcolor{red}{
The forecasting error measure ($E$ in Algorithm }\ref{alg:algo-lab})\textcolor{red}{ is adapted from the Overall Weighted Average error described in the M4 competition guide, which adds together the Mean Absolute Scaled Error and the symmetric Mean Absolute Percentage Error. For each series and method, the Mean Absolute Scaled Error and the symmetric Mean Absolute Percentage Error are divided by the respective error of the Naive 2 method *over all series in the dataset* (i.e. MASE by the average MASE of Naive 2), and then added.
}

\subsection{Metal-learning model
implementation}\label{metal-learning-model-implementation}

We choose the gradient tree boosting model of \textbf{xgboost}
(\textcite{chen2016xgboost}) as the underlying implementation of the
learning model due to following reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ability to customize the model to fit our objective function.
\item
  Computational complexity: The size of the M4 data set
  \textcolor{red}{imposes} an efficient model, for training an testing
  times.
\item
  Good performance in structure based problems, it is considered the
  state of the art model in machine learning challenges
  (\textcite{chen2016xgboost}).
\end{enumerate}

\subsubsection{The xgboost objective
function}\label{the-xgboost-objective-function}

The basic \textcolor{red}{xgboost} algorithm produces numeric values
from the features, one for each forecasting method in our pool. We apply
the \textcolor{red}{softmax} transform to these values prior to
computing the the objective function
\textcolor{red}{which includes the forecasting errors} shown in
Algorithm \ref{alg:algo-lab} step 7. This implemented as a \emph{custom
objective function} in the xgboost framework.

\textcolor{red}{xgboost requires a gradient and hessian of the objective function to fit the model.}
The \emph{correct} hessian is prone to numerical problems that need to
be addressed for \textcolor{red}{ the boosting} to converge. This is a
relative common problem and one simple fix is to use an upper bound of
the hessian by clamping its small values to a larger one. We computed a
\textcolor{red}{different} upper bound of the hessian by removing some
terms from the correct hessian. Although both alternatives converge, the
later works faster, requiring less boosting steps to converge. This not
only increases the computational efficiency, it also generalizes better
due to a less complex set of trees produced in the final solution.

The general paremeters of the meta-learning algorithm in
\ref{alg:algo-lab} are set to:

\begin{itemize}
\tightlist
\item
  \(p(\bm{f}_j)\) is the output of the xgboost algorithm for the
  features extrated from series \(X_j\).
\item
  \(y_j = \frac{exp(p(\bm{f}_j)_k)}{ \sum_k exp(p(\bm{f}_j)_k)}\) is the
  transformation to probabilities of the xgboost output by applying the
  softmax transform.
\item
  \(e_{jk}\) is the OWA error measure of forecasting method \(k\) in
  series \(j\).
\item
  \(L_j = \sum_{k=1}^K y_{jk} e_{jk}\) is the loss function, step 6 of
  Algorithm \ref{alg:algo-lab}.
\item
  \(G_j = \frac{\partial{L}}{\partial{y_j}} = y_j(e_j -\sum_k e_ky_k)\)
  is the gradient of the loss function.
\item
  The hessian \(H_j\) is approximated by our upper bound \(\hat{H_j}\):
  \[ H_j = \frac{\partial{G_j}}{\partial{y_j}} \approx \hat{H}_j = y_j(e_j(1-y_j) - G_j)  \]
\end{itemize}

The functions \(G\) and \(\hat{H}\) are passed to the xgboost algorithm
to minimize our objective function \(L\).

\subsubsection{Hyperparameters}\label{hyperparameters}

The results of xgboost are particularly dependent on its
hyper-parameters such as learning rate, number of boosting steps,
maximum complexity allowed for the trees or subsampling sizes. We limit
the hyper-parameter search space based on some initial results and rules
of thumb and explore it using Bayesian optimization, (implemented in the
\texttt{rBayesianOptimization} R package
\autocite{rBayesianOptimization}) measuring performance on a 10\%
holdout version of the reference set. We picked the simplest
hyper-parameter set from the top solutions of the exploration.

\subsection{Prediction Intervals}\label{prediction-intervals}

The M4 Competition also featured a sub competition over the accuracy of
prediction intervals. For this part, we used a different approach. In
order to compute the intervals we used the point forecast produced by
our meta-learner as the center of the interval and computed the 95\%
bounds of the interval by a linear combination of the bounds of three
forecasting methods: Thetaf, snaive and naive methods. The coefficients
for the linear combination were calculated in a data driven way also
over the M4 database. The procedure is as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For the training period of each series:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Compute the point forecast using FFORMA.
    \textcolor{red}{At this stage we do not use the full dataset for training FFORMA, since it would produce too acurate forecasts in the same dataset, leading to very small prediction intervals. we divide the M4 dataset in two parts, A and B. We train FFORMA on part A and use it to produce the point forecasts over the series of part B, and the reverse.}
  \item
    Compute the 95\% \emph{predicion radius} for the thetaf, snaive and
    naive, this is the difference between the 95\% upper bound and the
    point forecast for each forecast horizon.
  \end{enumerate}
\item
  For each forecast horizon:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Find the coefficients that minimize the MSIS error,
    \textcolor{red}{(defined in the M4 Competition guide)}, of the
    interval with the FFORMA point forecast as center and a linear
    combination of the radiuses of thetaf, snaive and naive as radius.
    The minimization is done by gradient descent over the test period of
    the series.
  \end{enumerate}
\end{enumerate}

This procedure produces a set of three coefficients for each prediction
horizon in the M4 dataset and these coefficients will be the same
independently of the series we want to forecast. \emph{Unlike the point
forecast, these coefficients are not restricted to be probabilities.}

\section{Results}\label{results}

We measure the impact of our contribution by comparing the average
forecasting error produced by our proposed combination model to a
reference implementation of a traditional classification approach, in
which a model is trained to pick the best method, which will then be
used for the final forecast. We use the same set of features, the same
pool of forecasting methods, the same underlying implementation
(xgboost), and in both approaches we perform hyperparameter search prior
to fitting the model using bayesian optimization. This way, we isolate
the model differences. In the M4 competition dataset for point forecast,
our approach produces a \textbf{relative improvement of 10\%} over the
reference, measured as
\(\frac{classifier\ error}{proposed\ method\ error}\).

\section{Discussion and Conclusions}\label{conclusion}

We have presented an algorithm for forecasting using weighted averaging
of a set of models. The objective function of the learning model assigns
probabilities to forecasting methods in order to minimize the
forecasting error that would be produced if we picked the methods at
random using these probabilities. This contrasts with how the final
forecasts are produced, which is a weighted average, not a selection.
These weight can however be used as model selection algorithm, if one
picks the methods receiving largest weight. This can be useful for
interpretability or computational reasons, at the cost of forecasting
performance.

One advantage of the approach is that its form is not independent of the
forecasting error measure, forcasting errors enter the model as
additional precalculated constants. This allows FFORMA to adapt to
arbitrary error measures when models that directly minimize them would
be restricted, e.g.~our approach can be applied to non-differentiable
errors.

We explored minimizing the weighted average of the forecasts, but the
results did not improve over the simple version.

The source code for FFORMA is available at
\url{https://github.com/robjhyndman/M4metalearning}.

\printbibliography[title=References]

\end{document}
