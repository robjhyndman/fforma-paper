\documentclass[11pt,a4paper,]{article}
\usepackage{lmodern}

\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdftitle={FFORMA: Feature-based FORecast-model Averaging},
            pdfkeywords={FFORMS (Feature-based FORecast-model Selection), Time series features,
Forecast combination, XGBoost, M4 Competition, Meta-learning},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{geometry}
\geometry{a4paper, text={16cm,24cm}}
\usepackage[style=authoryear-comp,]{biblatex}
\addbibresource{references.bib}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{long table}}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


\title{FFORMA: Feature-based FORecast-model Averaging}

%% MONASH STUFF

%% CAPTIONS
\RequirePackage{caption}
\DeclareCaptionStyle{italic}[justification=centering]
 {labelfont={bf},textfont={it},labelsep=colon}
\captionsetup[figure]{style=italic,format=hang,singlelinecheck=true}
\captionsetup[table]{style=italic,format=hang,singlelinecheck=true}

%% FONT
\RequirePackage{bera}
\RequirePackage{mathpazo}

%% HEADERS AND FOOTERS
\RequirePackage{fancyhdr}
\pagestyle{fancy}
\rfoot{\Large\sffamily\raisebox{-0.1cm}{\textbf{\thepage}}}
\makeatletter
\lhead{\textsf{\expandafter{\@title}}}
\makeatother
\rhead{}
\cfoot{}
\setlength{\headheight}{15pt}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\fancypagestyle{plain}{%
\fancyhf{} % clear all header and footer fields
\fancyfoot[C]{\sffamily\thepage} % except the center
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}}

%% MATHS
\RequirePackage{bm,amsmath}
\allowdisplaybreaks

%% GRAPHICS
\RequirePackage{graphicx}
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.8}

%\RequirePackage[section]{placeins}

%% SECTION TITLES
\RequirePackage[compact,sf,bf]{titlesec}
\titleformat{\section}[block]
  {\fontsize{15}{17}\bfseries\sffamily}
  {\thesection}
  {0.4em}{}
\titleformat{\subsection}[block]
  {\fontsize{12}{14}\bfseries\sffamily}
  {\thesubsection}
  {0.4em}{}
\titlespacing{\section}{0pt}{*5}{*1}
\titlespacing{\section}{0pt}{*2}{*0.2}


%% TITLE PAGE
\def\Date{\number\day}
\def\Month{\ifcase\month\or
 January\or February\or March\or April\or May\or June\or
 July\or August\or September\or October\or November\or December\fi}
\def\Year{\number\year}

\makeatletter
\def\wp#1{\gdef\@wp{#1}}\def\@wp{??/??}
\def\jel#1{\gdef\@jel{#1}}\def\@jel{??}
\def\showjel{{\large\textsf{\textbf{JEL classification:}}~\@jel}}
\def\nojel{\def\showjel{}}
\def\addresses#1{\gdef\@addresses{#1}}\def\@addresses{??}
\def\cover{{\sffamily\setcounter{page}{0}
        \thispagestyle{empty}%
        \vspace*{-2cm}
        \centerline{\raisebox{-1.8cm}{\includegraphics[width=5cm]{MBSportrait}}\hspace*{9cm} ISSN 1440-771X}\vspace{0.99cm}
        \begin{center}\Large
        Department of Econometrics and Business Statistics\\[.5cm]
        \scriptsize http://business.monash.edu/econometrics-and-business-statistics/research/publications
        \end{center}\vspace{2cm}
        \begin{center}
        \fbox{\parbox{14cm}{\begin{onehalfspace}\centering\Huge\vspace*{0.3cm}
                \textsf{\textbf{\expandafter{\@title}}}\vspace{1cm}\par
                \LARGE\@author\end{onehalfspace}
        }}
        \end{center}
        \vfill
                \begin{center}\Large
                \Month~\Year\\[1cm]
                Working Paper \@wp
        \end{center}}}
\def\pageone{{\sffamily
        \newpage%\blankpage
        \thispagestyle{empty}
        \vbox to 23cm{
        \raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}
        \vspace{2cm}\par
        \hspace{1cm}\parbox{14cm}{\sffamily\large\@addresses}\vspace{1cm}\vfill
        \hspace{1cm}{\large\Date~\Month~\Year}\\[1cm]
        \hspace{1cm}\showjel\vss}}}
\def\blindtitle{{\sffamily
     \thispagestyle{plain}\raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}\vspace{1cm}\par
        }}
\def\titlepage{{\cover\newpage\setstretch{1}\pageone\newpage\blindtitle}}

\def\blind{\def\titlepage{{\blindtitle}}\let\maketitle\blindtitle}
\def\titlepageonly{\def\titlepage{{\pageone\end{document}}}}
\def\nocover{\def\titlepage{{\newpage\setstretch{1}\pageone\newpage\blindtitle}}\let\maketitle\titlepage}
\let\maketitle\titlepage
\makeatother

%% SPACING
\RequirePackage{setspace}
\spacing{1.5}

%% LINE AND PAGE BREAKING
\sloppy
\clubpenalty = 10000
\widowpenalty = 10000
\brokenpenalty = 10000
\RequirePackage{microtype}

%% PARAGRAPH BREAKS
\setlength{\parskip}{1.4ex}
\setlength{\parindent}{0em}

%% HYPERLINKS
\RequirePackage{xcolor} % Needed for links
\definecolor{darkblue}{rgb}{0,0,.6}
\RequirePackage{url}

\makeatletter
\@ifpackageloaded{hyperref}{}{\RequirePackage{hyperref}}
\makeatother
\hypersetup{
     citecolor=0 0 0,
     breaklinks=true,
     bookmarksopen=true,
     bookmarksnumbered=true,
     linkcolor=darkblue,
     urlcolor=blue,
     citecolor=darkblue,
     colorlinks=true}

%% KEYWORDS
\newenvironment{keywords}{\par\vspace{0.5cm}\noindent{\sffamily\textbf{Keywords:}}}{\vspace{0.25cm}\par\hrule\vspace{0.5cm}\par}

%% ABSTRACT
\renewenvironment{abstract}{\begin{minipage}{\textwidth}\parskip=1.4ex\noindent
\hrule\vspace{0.1cm}\par{\sffamily\textbf{\abstractname}}\newline}
  {\end{minipage}}


\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage[showonlyrefs]{mathtools}
\usepackage[no-weekday]{eukdate}

%% BIBLIOGRAPHY

\makeatletter
\@ifpackageloaded{biblatex}{}{\usepackage[style=authoryear-comp, backend=biber, natbib=true]{biblatex}}
\makeatother
\ExecuteBibliographyOptions{bibencoding=utf8,minnames=1,maxnames=3, maxbibnames=99,dashed=false,terseinits=true,giveninits=true,uniquename=false,uniquelist=false,doi=false, isbn=false,url=true,sortcites=false}

\DeclareFieldFormat{url}{\texttt{\url{#1}}}
\DeclareFieldFormat[article]{pages}{#1}
\DeclareFieldFormat[inproceedings]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[incollection]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[article]{volume}{\mkbibbold{#1}}
\DeclareFieldFormat[article]{number}{\mkbibparens{#1}}
\DeclareFieldFormat[article]{title}{\MakeCapital{#1}}
\DeclareFieldFormat[article]{url}{}
%\DeclareFieldFormat[book]{url}{}
%\DeclareFieldFormat[inbook]{url}{}
%\DeclareFieldFormat[incollection]{url}{}
%\DeclareFieldFormat[inproceedings]{url}{}
\DeclareFieldFormat[inproceedings]{title}{#1}
\DeclareFieldFormat{shorthandwidth}{#1}
%\DeclareFieldFormat{extrayear}{}
% No dot before number of articles
\usepackage{xpatch}
\xpatchbibmacro{volume+number+eid}{\setunit*{\adddot}}{}{}{}
% Remove In: for an article.
\renewbibmacro{in:}{%
  \ifentrytype{article}{}{%
  \printtext{\bibstring{in}\intitlepunct}}}

\AtEveryBibitem{\clearfield{month}}
\AtEveryCitekey{\clearfield{month}}

\makeatletter
\DeclareDelimFormat[cbx@textcite]{nameyeardelim}{\addspace}
\makeatother
\renewcommand*{\finalnamedelim}{%
  %\ifnumgreater{\value{liststop}}{2}{\finalandcomma}{}% there really should be no funny Oxford comma business here
  \addspace\&\space}
  

\wp{no/yr}
\jel{C10,C14,C22}


\blind



\date{\sf\Date~\Month~\Year}
\makeatletter
 \lfoot{\sf\@date}
\makeatother

%% Any special functions or other packages can be loaded here.
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsthm}
\usepackage{amsmath,bm}
\usepackage{paralist}
\usepackage{todonotes}
\usepackage{ctable}

\def\Naive{Na\"{i}ve}
\def\naive{na\"{i}ve}
\def\sectionautorefname{Section}
\captionsetup[figure]{font=small}
\captionsetup[table]{font=small}
\def\var{\text{Var}}
\allowdisplaybreaks
\sloppy

%% LINE AND PAGE BREAKING
\clubpenalty = 4500
\widowpenalty = 4500
\brokenpenalty = 4500


\def\yes{$\checkmark$}

\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{5pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}


\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[section]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle
\begin{abstract}
\textcolor{red}{Needs to be re-written} To produce good forecasts of
time series, we must first solve the problem of which model to use.
Moreover, forecasting accuracy can even be improved by combining
different models. We present an automated method for combining
forecasting models that outperforms any individual method in a set of
the most popular ones in the time series forecasting literature,
achieving second position in the M4 Competition. Our approach works in
two phases. In the first, we use a dataset of time series to train a
meta-model to assign probabilities to the forecasting methods with the
goal of minimizing the average forecasting error. In the second phase,
we forecast new series by assigning probabilities to methods using our
previously trained meta-model, and then combine their individual
forecasts using a weighted average with these probabilities. The inputs
to the meta-model are features extracted from the series.
\end{abstract}
\begin{keywords}
FFORMS (Feature-based FORecast-model Selection), Time series features,
Forecast combination, XGBoost, M4 Competition, Meta-learning
\end{keywords}

\section{Introduction}\label{intro}

There are essentially two general approaches for forecasting a time
series: (i) generating forecasts from a single model, and (ii) combining
forecasts from many models or forecast model averaging. There has been a
vast literature on the latter motivated by the seminal work of
\textcite{Bates1969-cu} and followed by a plethora of empirical
applications showing that combination forecasts are often superior to
their individual counterparts \autocite[see among others,][
\textcolor{red}{maybe add more here}]{Cle1989}. Combining forecasts is
considered a successful way of hedging against the risk of selecting a
misspecified model. Of course the main challenge is in selecting an
appropriate set of weights with many contributions \autocite[see for
example,][]{Aiolfi2011-yz} attempting to overcome the so called
``forecast combination puzzle''.

There have been various efforts in the literature using time series
features combined with meta-learning for forecasting \autocites[see for
example][]{prudencio2004using}{LEMKE20102006}{KucEtAl2016}.
\textcolor{red}{(Leave this for Rob as a reminder: not sure how to include Kang et a. 2017 here).}
Recently \textcite{fforms} proposed FFORMS (Feature-based FORecast Model
Selection) a framework that focuses on using time series features
combined with meta-learning for forecast-model selection. In this paper
we build on this framework posing the learning problem in a way that
includes information about the forecast errors produced by the set of
candidate forecasting methods and combines these with features extracted
from the time series in order to derive a set of weights for forecast
combinations. We label this framework, FFORMA (Feature-based FORecast
Model Averaging). FFORMA resulted in the second most accurate point
forecasts and prediction intervals amongst all competitors in the M4
competition.

The rest of the paper is organized as follows: In \autoref{methodology}
we describe the FFORMA framework in a very general sense.
\autoref{M4application} gives the details of our implementation of
FFORMA in the M4 Competition for generating both point and interval
forecasts. This includes the required preprocessing steps, the set of
features and forecast methods, as well as the specific implementation of
the meta-learning model. We show empirical evidence on the performance
of the approach in \autoref{results} by quantifying the difference
between our proposed learning model and a traditional classifier
approach. \autoref{conclusion} provides some final remarks and
conclusions.

\section{Methodology}\label{methodology}

\subsection{Intuition and overview of
FFORMA}\label{intuition-and-overview-of-fforma}

The objective of our meta-learning approach is to derive a set of
weights to combine forecasts generated from a \emph{pool of methods}
(e.g., \naive, exponential smoothing, ARIMA, etc.). The FFORMA framework
requires a set of time series we refer to as the \emph{reference set}.
From these the algorithm learns in order to produce forecast combination
weights for any given time series. Each time series in the reference set
is divided into a training period and a test period. From the training
period a set of \emph{time series features} are calculated (e.g., length
of time series, strength of trend, etc.). These form the inputs for
training the meta-learning model. Moreover, each method in the pool is
fitted to the training period and forecasts are generated over the test
period. From these, \emph{forecast errors} are computed for each method
which form the supervised part for the training of the meta-learning
model. These are subsequently summarised using a forecast error measure.

The meta-learning model learns to produce weights for each method in the
pool as a function of the features of the series to be forecasted. Once
the model is trained, the inductive step consists of assuming that a new
series for which forecasts are required comes from a \emph{generating
process} similar to the one that generated the reference set.

A common meta-learning approach is to learn to select the best method in
the pool of methods for each series, i.e., the one that produces the
smallest forecast error. This approach transforms the problem into a
traditional classification problem by setting the individual forecasting
methods as the classes and the best method as the target class for each
time series. However, this simplification removes relevant information
which may be useful in the meta-learning process, such as, which methods
produce similar errors to the most accurate one, or which series are the
most difficult to forecast and hence have more impact in the total
forecast error. In this paper we do not apply this simplification but
instead take into account the exact error that each method produces.
This information is introduced into the model by posing the problem as
finding a function that assigns \emph{probabilities} to each forecasting
method, with the objective of minimizing the expected error that would
have been produced if the methods were picked at random following these
probabilities. This approach is more general than classification, and
can be thought of as classification with \emph{per class weights} (the
forecast errors) that vary per instance, combined with \emph{per
instance weights} that assign more importance to some series.

\subsection{Algorithmic description}\label{algorithmic-description}

The operation of the FFORMA framework comprises two phases: (i) the
offline phase, in which we train a meta-learner and (ii) the online
phase, in which we use the pre-trained meta-learner to identify forecast
combination weights. Algorithm \ref{alg:algo-lab} presents in detail the
Pseudo code of the proposed framework.

\begin{algorithm}[!ht]
  \caption{The FFORMA framework - Forecast combination based on meta-learning. }
  \label{alg:algo-lab}
  \begin{algorithmic}[1]
    \Statex \textbf{Offline phase - train the learning model}
    \Statex \text{Inputs:}
    \Statex \hspace{1cm}$\{x_1, x_2, \dots,x_N\}:$ $N$ observed time series forming the reference set.
         \Statex \hspace{1cm}$F:$ a set of functions for calculating time series features.
         \Statex \hspace{1cm}$M:$ a set of forecasting methods in the pool, e.g., \naive, ETS, ARIMA, etc.
           \Statex \hspace{1cm}$K:$ number of iterations in the xgboost algorithm.
     \Statex \text{Output:}
      \Statex \hspace{1cm}\text{FFORMA meta-learner: A function from the extracted features to a set of $M$}
      \Statex \text{\hspace{1cm} probabilities, one for each forecasting method.}\vspace{0.3cm}
    \Statex \textit{Prepare the meta-data} \vspace{0.1cm}
    \Statex For $n=1$ to $N$:
            \State Split $x_n$ into a training period and test period.
            \State Calculate the set of features $\bm{f}_n\in F$ over the training period.
            \State Fit each forecasting method $m\in M$ over the training period and generate forecasts over the test period.
            \State Calcuate forecast losses $L_{nm}$ over the test period.
    \State Meta-data: input features $\bm{f}_n$ (Step 2), output loss $L_{nm}$ (Step 4).
     \Statex
    \Statex \textit{Train the meta-learner, $w$}  \vspace{0.1cm}
            \State Train a learning model based on the meta-data and errors, by minimizing:
            $$ argmin_w \sum_{n=1}^N \sum_{m=1}^M w(x_n)_{m} L_{nm}$$ were $L$ is the loss function.
    \Statex
     \Statex \textbf{Online phase - forecast a new time series}
    \Statex \text{Input:}
    \Statex \hspace{1cm}\text{FFORMA classifier from Step 6}.
     \Statex \text{Output:}
      \Statex \hspace{1cm}\text{Forecast the new time series $x_{new}$}.
      \Statex For $x_{new}$: 
  \State Calculate features $\bm{f}_{new}$ by applying $F$.
  \State Use the meta-learner to produce $\bm{w}(\bm{f}_{new})$ an $M$-vector of probabilities.
  \State Compute the individual forecasts of the $M$ forecasting methods in the pool.
  \State Combine individual forecasts using $\bm{w}$ to generate final forecasts.
   \end{algorithmic}

\end{algorithm}

\clearpage

\section{Implementation and Application to M4
Competition}\label{M4application}

\subsection{Reference set}\label{reference-set}

The M4 dataset includes 100,000 real-world time series of yearly,
quarterly, monthly, weekly, daily and hourly data. All 100,000 series
form the reference set. Each series is split into training period and a
test period. The length of the test period for each time series was set
to be equal to the forecast horizon set by the competition. When the
resulting training period was too short (fewer than two observations
\textcolor{red}{I have left this as observations to avoid confusion with train and test period}
\textcolor{green}{ok, also it is not a very important detail, we can remove the whole sentence.}),
the size of the test period was reduced. The series that were simply a
constant over the training period were eliminated from any further
analysis.

\subsection{Time series features}\label{time-series-features}

\autoref{feature} provides a brief description of the features used in
this experiment, \(F\) in Algorithm \ref{alg:algo-lab}. The functions to
calculate these are implemented in the \texttt{tsfeatures} R package by
\textcite{tsfeatures}. Most of the features (or variations of these)
have been previously used in a forecasting context by
\textcite{hyndman2015large} and \textcite{fforms} (please refer to these
papers for detailed descriptions). The ARCH.LM statistic is calculated
based on the Lagrange Multiplier test of
\textcite{engle1982autoregressive} for autoregressive conditional
heteroscedasticity (ARCH). For the heterogeneity features 39-42, the
time series is first pre-whitened using an AR model resulting to a new
series \(z\). A GARCH(1, 1) model is then fitted to the pre-whitened
series to obtain the residual series, \(r\). Subsequently, the features
are computed.

Features corresponding to seasonal time series only, are set to zero for
the case of non-seasonal time series. Note that for the sake of
generality, we have not used any of the domain-specific features such as
macro, micro, finance, etc., even though this information was available
in the M4 data set.

\begin{table}[!htp]
\centering\footnotesize\tabcolsep=0.12cm
\caption{Features used in FFORMA framework.}
\label{feature}
\begin{tabular}{llp{8,8cm}cc}
\toprule
\multicolumn{2}{c}{Feature} & Description & Non-seasonal & Seasonal\\
\midrule
1  & T              & length of time series                                                                   & \yes  & \yes \\
2  & trend          & strength of trend                                                                       & \yes  & \yes\\
3  & seasonality    & strength of seasonality                                                                 & -     & \yes \\
4  & linearity      & linearity                                                                               & \yes  & \yes \\
5  & curvature      & curvature                                                                               & \yes  & \yes \\
6  & spikiness      & spikiness                                                                               & \yes  & \yes \\
7  & e\_acf1        & first ACF value of remainder series                                                     & \yes  & \yes \\
8  & e\_acf10       & sum of squares of first 10 ACF values of remainder series                                                    & \yes  & \yes \\
9  & stability      & stability                                                                               & \yes  & \yes \\
10  & lumpiness      & lumpiness                                                                               & \yes  & \yes \\
11 & entropy        & spectral entropy                                                                        & \yes  & \yes \\
12 & hurst          & Hurst exponent                                                                          & \yes  & \yes \\
13 & nonlinearity   & nonlinearity                                                                            & \yes\ & \yes \\
13 & alpha          & ETS(A,A,N) $\hat\alpha$                                                                 & \yes  & \yes \\
14 & beta           & ETS(A,A,N) $\hat\beta$                                                                  & \yes  & \yes\\
15 & hwalpha        & ETS(A,A,A) $\hat\alpha$                                                                 & -     & \yes \\
16 & hwbeta         & ETS(A,A,A) $\hat\beta$                                                                  & -     & \yes \\
17 & hwgamma        & ETS(A,A,A) $\hat\gamma$                                                                 & -     & \yes \\
18 & ur\_pp         & test statistic based on Phillips-Perron test                                            & \yes  & \yes \\
19 & ur\_kpss       & test statistic based on KPSS test                                                       & \yes  & \yes \\
20 & y\_acf1        & first ACF value of the original series                                                  & \yes  & \yes \\
21 & diff1y\_acf1   & first ACF value of the differenced series                                               & \yes  & \yes \\
22 & diff2y\_acf1   & first ACF value of the twice-differenced series                                         & \yes  & \yes \\
23 & y\_acf10        & sum of squares of first 10 ACF values of original series                                 & \yes  & \yes \\
24 & diff1y\_acf10   & sum of squares of first 10 ACF values of differenced series                              & \yes  & \yes \\
25 & diff2y\_acf10   & sum of squares of first 10 ACF values of twice-differenced series                        & \yes  & \yes \\
26 & seas\_acf1     & autocorrelation coefficient at first seasonal lag                                       & -     & \yes \\
27 & sediff\_acf1   & first ACF value of seasonally-differenced series                                        & -     & \yes\\
28 & y\_pacf5       & sum of squares of first 5 PACF values of original series                                & \yes  & \yes \\
29 & diff1y\_pacf5  & sum of squares of first 5 PACF values of differenced series                             & \yes  & \yes \\
30 & diff2y\_pacf5  & sum of squares of first 5 PACF values of twice-differenced series                       & \yes  & \yes \\
31 & seas\_pacf  & partial autocorrelation coefficient at first seasonal lag                       & \yes  & \yes \\
32 & crossing\_point  & number of times the time series crosses the median                     & \yes  & \yes \\
33 & flat\_spots  & number of flat spots, calculated by discretizing the series into 10 equal sized intervals and counting the maximum run length within any single interval                       & \yes  & \yes \\
34 & nperiods  & number of seasonal periods in the series & -  & \yes \\
35 & seasonal\_period  & length of seasonal period                       & -  & \yes \\
36 & peak  & strength of peak                      & \yes  & \yes \\
37 & trough  & strength of trough                      & \yes  & \yes \\
38 & ARCH.LM  & ARCH LM statistic                      & \yes  & \yes \\
39 & arch\_acf  &    sum of squares of the first 12 autocorrelations of $z^2$               & \yes  & \yes \\
40 & garch\_acf  &  sum of squares of the first 12 autocorrelations of $r^2$                 & \yes  & \yes \\
41 & arch\_r2  &     $R^2$ value of an AR model applied to $z^2$              & \yes  & \yes \\
42 & garch\_r2  &   $R^2$ value of an AR model applied to \textcolor{blue}{$r^2$}               & \yes  & \yes \\
\bottomrule
 \end{tabular}
\end{table}

\clearpage

\subsection{Pool of forecasting
methods}\label{pool-of-forecasting-methods}

We consider nine methods implemented in the \texttt{forecast} package in
R \autocite{forecast} for the pool of methods, \(P\) in Algorithm
\ref{alg:algo-lab}. They are (the specific R calls for fitting the
models are given):

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\tightlist
\item
  \naive~(\texttt{naive});
\item
  random walk with drift (\texttt{rwf} with drift=TRUE);
\item
  seasonal \naive~(\texttt{snaive}).
\item
  theta method (\texttt{thetaf});
\item
  automated ARIMA algorithm (\texttt{auto.arima});
\item
  automated exponential smoothing algorithm (\texttt{ets});
\item
  TBATS model (\texttt{tbats});
\item
  STLM-AR Seasonal and Trend decomposition using Loess with AR modeling
  of the seasonally adjusted series (\texttt{stlm} with model function
  \texttt{ar});
\end{enumerate}

In all cases, the default settings are used. Further, in the case of an
error when fitting the series (e.g.~a series is constant), the
\texttt{snaive} forecast method is used instead.

\subsection{Forecast error measure}\label{forecast-error-measure}

\textcolor{red}{We don't need this at all as we stipulate the $L$  below. Let's remove this.}

\textcolor{green}{We can remove the whole section, there is only one small detail:
The OWA is defined in the guide as a measure for the whole dataset, what we use in FFORMA is the *contribution* to the OWA of each series.
Added the clarification in $L$ below. Do you think is enough?
Also, I think the OWA as written in the in the M4 guide *may* not be very clear, personally, I ended up going to the source code they provided.
}

The forecasting error measure, \(E\) in Algorithm \ref{alg:algo-lab}, is
adapted from the Overall Weighted Average error described in the M4
competition guide, which adds together the Mean Absolute Scaled Error
and the symmetric Mean Absolute Percentage Error. For each series and
method, the Mean Absolute Scaled Error and the symmetric Mean Absolute
Percentage Error are divided by the respective error of the Naive 2
method \emph{over all series in the dataset} (i.e.~MASE by the average
MASE of Naive 2), and then added.

\subsection{Metal-learning model
implementation}\label{metal-learning-model-implementation}

We use the gradient tree boosting model of \texttt{xgboost} as the
underlying implementation of the learning model. This is a state of the
art computationally efficient model that has shown good performance in
structure based problems \autocite{chen2016xgboost}. The great advantage
here is that we are able to customise the model to fit our objective
function.

\subsubsection{The xgboost objective
function}\label{the-xgboost-objective-function}

The basic xgboost algorithm produces numeric values from the features,
one for each forecasting method in our pool. We apply the softmax
transform to these values prior to computing the the objective function
which includes the forecasting errors shown in Algorithm
\ref{alg:algo-lab} Step 6. This implemented as a \emph{custom objective
function} in the xgboost framework.

xgboost requires a gradient and hessian of the objective function to fit
the model. The \emph{correct} hessian is prone to numerical problems
that need to be addressed for the boosting to converge. This is a
relative common problem and one simple fix is to use an upper bound of
the hessian by clamping its small values to a larger one. We computed a
different upper bound of the hessian by removing some terms from the
correct hessian. Although both alternatives converge, the later works
faster, requiring less boosting steps to converge. This not only
increases the computational efficiency, it also generalizes better due
to a less complex set of trees produced in the final solution.
\textcolor{red}{I am not sure about the above. I leave to Rob to evaluate.}

The general paremeters of the meta-learning in Algorithm
\ref{alg:algo-lab} are set to:

\begin{itemize}
\tightlist
\item
  \(p(\bm{f}_n)_m\) is the output of the xgboost algorithm, one value
  for each forecasting method m, for the features extracted from series
  \(x_n\).
\item
  \(w(x_n)_{m} = \frac{exp(p(\bm{f}_n)_m)}{ \sum_m exp(p(\bm{f}_j)_m)}\)
  is the transformation to probabilities of the xgboost output by
  applying the softmax transform.
\item
  \(L_{nm}\) is the contribution to the OWA error measure of method m
  for the series n.
\item
  \(\bar{L}_n = \sum_{m=1}^M w(x_n)_m L_{nm}\) is the weighted average
  loss function, Step 6 of Algorithm \ref{alg:algo-lab}.
\item
  \textcolor{red}{I am not sure at all how the following two are derived. Rob was not sure either. Pablo can you please help us understand here.}
  \(G_n = \frac{\partial{\bar{L}}}{\partial{p(\bm{f}_n)}} = w_n(L_n -\sum_m L_mw_m)\)
  is the gradient of the loss function.
  \textcolor{green}{!!The denominator of the derivative was incorrect. I wanted to show the gradient with respect to the xgboost prediction, $p$, not after the softmax transform, $w$ (which is only added because it is very convenient notation-wise).
    \\The formula is written in a way that is computationally efficient, in "vector form" and to reuse many terms, maybe that is why it is not straightforward.
    \\ NOTATION WISE, Do you think it is OK or should we use
    $\frac{\partial{\bar{L}_n}}{\partial{p_n}}$}
\item
  The hessian \(H_n\) is approximated by our upper bound \(\hat{H_n}\):
  \[ H_n = \frac{\partial{G_n}}{\partial{p(\bm{f}_n)}} \approx \hat{H}_n = w_n(L_n(1-w_n) - G_n)  \]
\end{itemize}

The functions \(G\) and \(\hat{H}\) are passed to the xgboost algorithm
to minimize our objective function \(L\).

\subsubsection{Hyperparameters}\label{hyperparameters}

The results of xgboost are particularly dependent on its
hyper-parameters such as learning rate, number of boosting steps,
maximum complexity allowed for the trees or subsampling sizes. We limit
the hyper-parameter search space based on some initial results and rules
of thumb and explore it using Bayesian optimization
\autocite[implemented in the \texttt{rBayesianOptimization} R
package][]{rBayesianOptimization} measuring performance on a 10\%
holdout version of the reference set. We picked the simplest
hyper-parameter set from the top solutions of the exploration.

\subsection{Prediction Intervals}\label{prediction-intervals}

For each series \(x_{new}\) we use as the center of the interval the
point forecast produced by our meta-learner. Then the 95\% bounds of the
interval are generated by a linear combination of the bounds of three
forecasting methods: \naive, theta and seasonal \naive~. The
coefficients for the linear combination are calculated in a data driven
way over the M4 database. The complete procedure is as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We divide the M4 dataset into two parts: A and B. We train the FFORMA
  learner using the training periods of the series in part A and produce
  point forecasts over the \textcolor{green}{training periods of the}
  series of part B, and vice versa.
\item
  We compute the 95\% \emph{predicion radius} for the \naive, theta, and
  seasonal \naive~methods. This is the difference between the 95\% upper
  bound and the point forecast for each forecast horizon.
\item
  For each forecast horizon we find the coefficients that minimize the
  MSIS of the interval, as defined in the M4 Competition guide, with the
  FFORMA point forecast as the center and a linear combination of the
  radiuses of \naive, theta, seasonal \naive~forecasts as the interval.
  The minimization is done by gradient descent over the test period of
  the series.
\end{enumerate}

This method produces a set of three coefficients for each prediction
horizon in the M4 dataset and these coefficients will be the same
independently of the series we want to forecast. Unlike the point
forecast, these coefficients are not restricted to be probabilities.

\section{Results}\label{results}

In order to evaluate the impact of our contribution we compared the
average forecast error produced by FFORMA with a \emph{model selection}
approach.

In model selection meta-learning, a classifier is trained to predict the
``best method'' for each series based on the features instead the FFORMA
scheme of producing probabilities to minimize overall OWA. The final
forecasts are the ones produced by method in the pool predicted by the
classifier to be the best, not by averaging.

All other implementation details are kept the same as in FFORMA,
specifically it uses the same set of features, the same pool of
forecasting methods, the same underlying implementation (xgboost) and
the same hyperparameter search. This enables to measure the impact of
the FFORMA original loss function, with a ``model selection approach'',
all other things being equal.

We applied both FFORMA and the \emph{model selection approach} to the M4
dataset and compared their overall point forecast errors. The error of
the FFORMA approach is 10\% smaller than model selection, measured as
\(\frac{model\ selection\ OWA}{FFORMA \ OWA}\).
\textcolor{green}{SECTION HAS BEEN REWRITTEN}
\textcolor{red}{Pablo can you elaborate a little here. A couple more sentences shoud be enough. It is not exactly clear to me what this does. What you have on the comments below are very interesting and I think really worth writing up in order to understand/evaluate FFORMA. I think it would really be worth putting all this work in a seprate paper. I cannot see why the IJF would not be interested in it.}

\section{Discussion and Conclusions}\label{conclusion}

We have presented an algorithm for forecasting using weighted averaging
of a set of models. The objective function of the learning model assigns
probabilities to forecasting methods in order to minimize the
forecasting error that would be produced if we picked the methods at
random using these probabilities. This contrasts with how the final
forecasts are produced, which is a weighted average, not a selection.
These weights can however be used as model selection algorithm, if one
picks the methods receiving the largest weight. This can be useful for
interpretability or computational reasons, at the cost of forecasting
performance.

One advantage of the approach is that its form is not independent of the
forecasting error measure. Forecasting errors enter the model as
additional precalculated constants. This allows FFORMA to adapt to
arbitrary error measures when models that directly minimize them would
be restricted, e.g.~our approach can be applied to non-differentiable
errors.

We explored minimizing the weighted average of the forecasts, but the
results did not improve over the simple version.

The source code for FFORMA is available at
\url{https://github.com/robjhyndman/M4metalearning}.

\printbibliography[title=References]

\end{document}
