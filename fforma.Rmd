---
title: "FFORMA: Feature-based FORecast-model Averaging"
author:
- familyname: Pablo
  othernames: 
  address: 
  email: 
  correspondingauthor: true
- familyname: 
  othernames: 
  address: 
abstract: To produce good forecasts of time series, we must first solve the problem of which model to use. Moreover, forecasting accuracy can even be improved by combining different models. We present an automated method for combining forecasting models that outperforms any individual method in a set of the most popular ones in the time series forecasting literature, achieving second position in the M4 Competition. Our approach works in two phases. In the first, we use a dataset of time series to train a meta-model to assign probabilities to the forecasting methods with the goal of minimizing the average forecasting error. In the second phase, we forecast new series by assigning probabilities to methods using our previously trained meta-model, and then combine their individual forecasts using a weighted average with these probabilities. The inputs to the meta-model are features extracted from the series.

keywords: FFORMS (Feature-based FORecast-model Selection), Time series features, Forecast combination, XGBoost, M4 Competition
wpnumber: no/yr
jelcodes: C10,C14,C22
blind: true
cover: true
toc: false
bibliography: references.bib
biblio-style: authoryear-comp
output:
  MonashEBSTemplates::workingpaper:
    fig_caption: yes
    fig_height: 5
    fig_width: 8
    includes:
      in_header: preamble.tex
    keep_tex: yes
    number_sections: yes
    citation_package: biblatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache=TRUE, messages=FALSE, warning=FALSE)
# Make sure you have the latest version of rmarkdown and bookdown
#devtools::install_github("rstudio/rmarkdown")
#devtools::install_github("rstudio/bookdown")
library(ggplot2)
```
<!-- OLD ABSTRACT: It is well known that ensemble approaches produce improvements over single methods in statistical learning. Nevertheless, when calculating predictions over a large dataset, computation time for the whole ensemble can be prohibitive, so individual model selection becomes the preferred approach. We present a method for combining forecasting models by posing it as a classification problem using features extracted from the time series. Unlike regular classification problems, we minimize the average forecast error of the selected method rather than the classification error. Not only does this address the aim of accurate forecasting, it also provides measures of relative method accuracy across the time series, and relative difficulty across time series. In contrast, a classic classification approach would give the same importance to all series and methods. The presented classifier is compared with state-of-the-art approaches to forecasting and time series classification. The results show an improvement of error over alternative approaches. These experiments allow us to show the relevance of both the feature set and the proposed optimization approach to several collections of time series. The scalability of the approach allows it to be applied to forecasting a large collection of time series. It can also be efficiently trained to tailor specific domains or datasets. -->


# Introduction{#intro}

There are essentially two general approaches for forecasting a time series: (i) generating forecasts from a single model, and (ii) combining forecasts from many models or forecast model averaging. There has been a vast literature on the latter motivated by the seminal work of @Bates1969-cu and followed by a plethora of empirical applications showing that combination forecasts are often superior to their individual counterparts (see among others, @clemen1989combining, \todo{maybe add more here}). Combining forecasts is considered a successful way of hedging against the risk of selecting a misspecified model. Of course the main challenge is in selecting an appropriate set of weights with many contributions (see for example Timmermann 2006, @Aiolfi2011-yz) attempting to overcome the so called ``forecast combination puzzle''. \todo{Pablo to add on the literature on ensemble methods}

Recently, @fforms have explored the idea of using time series features combined with meta-learning for forecast-model selection; a framework labelled as FFORMS (Feature-based FORecast Model Selection). In this paper we build on this framework posing the learning problem in a way that includes information about the forecast errors produced by the set of forecasting methods considered and combines these with features extracted from the time series in order to derive a set of weights for forecast combinations. We label this framework FFORMA (Feature-based FORecast Model Averaging). The proposed FFORMA framework has been used over the course of the M4 competition and resulted in the second most accurate point forecasts and prediction intervals amongst all competitors.

The rest of the paper is organized as follows: In \autoref{methodology} we describe the FFORMA framework in a very general sense. \autoref{M4application} gives the details of our implementation of FFORMA in the M4 Competition for generating both point and interval forecasts. This includes the required preprocessing steps, the set of features and forecast methods, as well as the specific implementation of the meta-learning model. We show empirical evidence on the performance of the approach in \autoref{results} by quantifying the difference between our proposed learning model and a traditional classifier approach. \autoref{conclusion}  provides some final remarks and conclusions.

# Methodology {#methodology}

## Overview / Intuition of FFORMA

The objective of our meta-learning approach is to derive a set of weights to combine forecasts generated from a *pool of methods* (e.g., na?ve, ETS, ARIMA, etc.). The FFORMA framework requires a set of time series we refer to as the *reference set*. From these the algorithm learns in order to produce forecast combination weights for any given time series.

Each time series in the reference set is divided into a training period and a test period. From the training period a set of *time series features* are calculated (e.g., length of time series, strength of trend, etc.). These form the inputs for training the meta-learning model. Moreover, each method in the pool is fitted to the training period and forecasts are generated over the test period. From these  a *forecast error measure* is computed for each method. These form the \textcolor{red}{ supervised part for the training of the meta-learning model.} (\textcolor{blue}{(Not sure if calling these labels may be confusing)labels ?)} } 
<!--The forecasting errors of each method in the pool are calculated for each time series in the reference set, using the true future values of the series if available or by dividing the each series into a training period and test period by applying temporal holdout.-->

The meta-learning model learns to produce weights for each method in the pool as a function of the features of the series. Once the model is trained, the inductive step consist of assuming that a new series for which we require forecasts comes from a *generating process* similar to the one that generated the reference set.

A common meta-learning approach is to learn to select the best method in the pool of methods for each series, i.e., the one that produces the smallest forecast error. This approach transforms the problem into a traditional classification problem by setting the individual forecasting methods as the classes and the best method as the target class for each time series, enabling the use of existing classification algorithms. This is what was implemented by @fforms and formed the basis of the FFORMS approaph.

However, this simplification removes relevant information which may be useful in the meta-learning process, such as which methods produce similar errors to the most accurate one, or which series \textcolor{red}{ have more impact in the total error} \textcolor{blue}{( are more *difficult* than others)} \todo{Not sure what difficult means here and how it is used - difficult to forecast? and then?}. In this paper we advance the FFORMS framework by not applying this simplification and taking into account the exact error that each method produces in each series, instead of just considering the one that produces the minimum error.

This information is introduced into the model by posing the problem as finding a function that assigns *probabilities* to each forecasting method for each series, with the objective of minimizing the expected error that is produced if the methods were picked at random following these probabilities.

\textcolor{red}{This approach approach is more general than classification, and can be thought of as classification  with *per class weights* (the forecasting errors) that vary per instance, combined with *per instance weights* that assign more importance to some series.}

\textcolor{blue}{
Our approach can be easily transformed into a classification exercise by selecting the method corresponding to the largest probability produced by our model. If we allow a sufficiently rich hypothesis space to fit freely, we also end up assigning probability one to the method with the smallest error. Hence our approach can also be seen as a classification exercise, but with *per class weights* (the forecasting errors) that vary per instance, combined with *per instance weights* that assign more importance to some series.}\todo{I think a lot of this - even all of it - can go if we need more space.}

<!-- The meta-learning process can therefore be summarized the following way: -->

<!--   * $X_i$ the $i$-eth series on the regerence set. -->
<!--   * $e_{ij}$ the error the individual forecasting method $j$ makes on series $i$. -->
<!--   * $x_{i}$ the feature vector extracted from series $X_i$. -->
<!--   * $f$ is a function belonging to a hypothesis space $H$ (such as linear functions) that maps features to probabilities, such as $f(x)_j$ is the probabilitie assigned to the $j$-eth method in the pool. -->

<!-- $$ argmin_{f \in H} \sum_i \sum_j f(x_{i})_je_{ij} $$ -->


<!--  Given a new time series, a trained meta-learning algorithm produces probabilities for each of the forecasting methods considered in the pool. The final forecast can be computed from these probabilities in two-ways: -->

<!--  1. A weighted average of the individual forecast methods. -->
<!--  This tends to produce the best forecast on average, but requires computing the forecasts of all methods in out pool, which may be time consuming. -->
<!--  2. Selecting the method with the largest probability. This prevents the need to calculate all the forecast of the individual methods in our pool, but produces less acurate forecasts on average. -->

## Algorithmic description

The operation of the FFORMA framework comprises two phases: (i) offline phase, in which we train a meta-learner and (ii) online phase, in which we use the pre-trained meta-learner to identify forecast combination weights. Algorithm \@ref(alg:algo-lab) presents in detail the Pseudo code of the proposed framework. 

<!--
  1. The set of forecasting methods, the pool of methods.
  2. The set of features to be extracted from the time series
  3. A training set of time series, the reference set.
  4. The forecasting error measure, such as mean squared error.

The FFORMA framework works in two phases: an **offline** phase, when the meta-learned is trained and an **online** phase, when forecasts are produced.  describes the two phases. -->


\begin{algorithm}[!ht]
  \caption{The FFORMA framework - Forecast combination based on meta-learning. }
  \label{alg:algo-lab}
  \begin{algorithmic}[1]
    \Statex \textbf{Offline phase - train the learning model}
    \Statex \text{Given:}
    \Statex \hspace{1cm}$\{X_1, X_2, \dots,X_N\}:$ $N$ observed time series forming the reference set.
      \Statex \hspace{1cm}$P:$ the pool of $K$ forecasting methods, e.g., na?ve, ETS, ARIMA, etc.
         \Statex \hspace{1cm}$F:$ the set of functions for calculating time series features.
         \Statex \hspace{1cm}$E:$ A forecast error measure, e.g., MASE, sMAPE, etc.
           \Statex \hspace{1cm}$L:$ Loss function.
           \Statex \hspace{1cm}$M:$ number of iterations in the xgboost algorithm.
     \Statex \text{Output:}
      \Statex \hspace{1cm}\text{FFORMA meta-learner: A function from the extracted features }
      \Statex \text{\hspace{1cm} to a set of $K$ probabilities, one for each method in $P$.}
    \Statex \textit{Prepare the meta-data}
    \Statex For $j=1$ to $N$:
            \State Split $X_j$ into a training period and test period.
            \State Calculate the set of features $\bm{f}_j$ over the training period by applying $F$.
            \State Fit each method in $P$ over the training period and generate forecasts over the test period.
            \State Calcuate forecast error measure  $e_{jk}$ over the test period for each method $k=1,\ldots, K$ in $P$.
 \textcolor{green}{\State Meta-data: input features $\bm{f}_j$ (step 2), output errors: $\bm{e}_{jk}$ (step 5).
     \Statex
    \Statex \textit{Train the meta-learner, $y$}
            \State Train a learning model based on the meta-data and errors, by minimizing:
            $$ argmin_y \sum_{j=1}^N \sum_{k=1}^K y(x_j)_k e_{jk}  $$
            \State {Meta-learner, $y$}.}
    \Statex
     \Statex \textbf{Online phase - forecast a new time series}
    \Statex \text{Given:}
    \Statex \hspace{1cm}\text{FFORMA classifier from step 7} .
     \Statex \text{Output:}
      \Statex \hspace{1cm}\text{Forecast for the new time series $X_{new}$}.
  \State For $X_{new}$ calculate features $\bm{f}_{new}$ by applying $F$.
  \State From the features, use the meta-learner to produce $y(\bm{f}_{new})=\bf{w}$ the vector of probabilities. 
  \State Compute the individual forecasts of the methods in $P$ for $X_{new}$
  \State Combine individual forecasts using $\bm{w}$ to generate final forecasts for {$X_{new}$}.
   \end{algorithmic}

\end{algorithm}


<!-- The algorithm produces weights for each forecasting method. These weights can also be interpreted as the probability of each method being best. These weights can be used either to select the "best" forecasting method for each series or to combine the forecasts using weighted linear combination. Note that the accuracy of the FFORMA meta-learner depends on three main factors: -->

<!--   1. Forecasting methods used in the pool -->
<!--   2. The set of time series features we considered -->
<!--   3. The collection of time series used to train the classifier. -->

<!--   Section 3 provides a more detailed description of application of the FFORMA framework over the course of M4 competition.  -->

\clearpage

#Implementation and Application to M4 Competition {#M4application}


## Reference set 

The M4 dataset includes 100,000 real-world time series of yearly, quarterly, monthly, weekly, daily and hourly data. All 100,000 series form the reference set. Each series is split into training period and a test period. The length of the test period for each time series was set to be equal to the forecast horizon set by the competition. When the resulting training period was too short (fewer than 2 \textcolor{red}{periods}\textcolor{blue}{(observations)}), the size of the test period was reduced. The series that were simply a constant over the training period were eliminated from any further analysis.

## Time series features

\autoref{feature} provides a brief description of the 42 features used in this experiment, \textcolor{red}{$F$ in Algorithm} \@ref(alg:algo-lab). The functions to calculate these are implemented in the `tsfeatures` R package by @tsfeatures. Most of the features have been previously used by @hyndman2015large and @fforms.  The last five heterogeneity features are calculated as follows. First, the time series is pre-whitened using an AR model resulting to a new series $z$. A GARCH(1, 1) model is then fitted to the pre-whitened series to obtain the residual series, $r$. Subsequently, features 39-42 are computed. The ARCH.LM statistic is calculated based on Lagrange Multiplier test of @engle1982autoregressive for autoregressive conditional heteroscedasticity (ARCH). A detailed description of the other features is provided in @fforms. The features corresponding to seasonal time series only, are set to zero for the case of non-seasonal time series. Note that, for the sake of generality, we have not used any of the domain-specific features such as macro, micro, finance, etc., even though this information was available in the M4 data set.\todo{Should we add here in an effort to be as general as possible.}

<!--No exogenous features were used, even though they were available in the M4 dataset, such as which domain the series belongs to (e.g. macroeconomics, finance, tourism...).--> 

\begin{table}[!htp]
\centering\footnotesize\tabcolsep=0.12cm
\caption{Features used in FFORMA framework.}
\label{feature}
\begin{tabular}{llp{8,8cm}cc}
\toprule
\multicolumn{2}{c}{Feature} & Description & Non-seasonal & Seasonal\\
\midrule
1  & T              & length of time series                                                                   & \yes  & \yes \\
2  & trend          & strength of trend                                                                       & \yes  & \yes\\
3  & seasonality    & strength of seasonality                                                                 & -     & \yes \\
4  & linearity      & linearity                                                                               & \yes  & \yes \\
5  & curvature      & curvature                                                                               & \yes  & \yes \\
6  & spikiness      & spikiness                                                                               & \yes  & \yes \\
7  & e\_acf1        & first ACF value of remainder series                                                     & \yes  & \yes \\
8  & e\_acf10       & sum of squares of first 10 ACF values of remainder series                                                    & \yes  & \yes \\
9  & stability      & stability                                                                               & \yes  & \yes \\
10  & lumpiness      & lumpiness                                                                               & \yes  & \yes \\
11 & entropy        & spectral entropy                                                                        & \yes  & \yes \\
12 & hurst          & Hurst exponent                                                                          & \yes  & \yes \\
13 & nonlinearity   & nonlinearity                                                                            & \yes\ & \yes \\
13 & alpha          & ETS(A,A,N) $\hat\alpha$                                                                 & \yes  & \yes \\
14 & beta           & ETS(A,A,N) $\hat\beta$                                                                  & \yes  & \yes\\
15 & hwalpha        & ETS(A,A,A) $\hat\alpha$                                                                 & -     & \yes \\
16 & hwbeta         & ETS(A,A,A) $\hat\beta$                                                                  & -     & \yes \\
17 & hwgamma        & ETS(A,A,A) $\hat\gamma$                                                                 & -     & \yes \\
18 & ur\_pp         & test statistic based on Phillips-Perron test                                            & \yes  & \yes \\
19 & ur\_kpss       & test statistic based on KPSS test                                                       & \yes  & \yes \\
20 & y\_acf1        & first ACF value of the original series                                                  & \yes  & \yes \\
21 & diff1y\_acf1   & first ACF value of the differenced series                                               & \yes  & \yes \\
22 & diff2y\_acf1   & first ACF value of the twice-differenced series                                         & \yes  & \yes \\
23 & y\_acf10        & sum of squares of first 10 ACF values of original series                                 & \yes  & \yes \\
24 & diff1y\_acf10   & sum of squares of first 10 ACF values of differenced series                              & \yes  & \yes \\
25 & diff2y\_acf10   & sum of squares of first 10 ACF values of twice-differenced series                        & \yes  & \yes \\
26 & seas\_acf1     & autocorrelation coefficient at first seasonal lag                                       & -     & \yes \\
27 & sediff\_acf1   & first ACF value of seasonally-differenced series                                        & -     & \yes\\
28 & y\_pacf5       & sum of squares of first 5 PACF values of original series                                & \yes  & \yes \\
29 & diff1y\_pacf5  & sum of squares of first 5 PACF values of differenced series                             & \yes  & \yes \\
30 & diff2y\_pacf5  & sum of squares of first 5 PACF values of twice-differenced series                       & \yes  & \yes \\
31 & seas\_pacf  & partial autocorrelation coefficient at first seasonal lag                       & \yes  & \yes \\
32 & crossing\_point  & number of times the time series crosses the median                     & \yes  & \yes \\
33 & flat\_spots  & number of flat spots, calculated by discretizing the series into 10 equal sized intervals and counting the maximum run length within any single interval                       & \yes  & \yes \\
34 & nperiods  & number of seasonal periods in the series & -  & \yes \\
35 & seasonal\_period  & length of seasonal period                       & -  & \yes \\
36 & peak  & strength of peak                      & \yes  & \yes \\
37 & trough  & strength of trough                      & \yes  & \yes \\
38 & ARCH.LM  & ARCH LM statistic                      & \yes  & \yes \\
39 & arch\_acf  &    sum of squares of the first 12 autocorrelations of $z^2$               & \yes  & \yes \\
40 & garch\_acf  &  sum of squares of the first 12 autocorrelations of $r^2$                 & \yes  & \yes \\
41 & arch\_r2  &     $R^2$ value of an AR model applied to $z^2$              & \yes  & \yes \\
42 & garch\_r2  &   $R^2$ value of an AR model applied to \textcolor{blue}{$r^2$}               & \yes  & \yes \\
\bottomrule
 \end{tabular}
\end{table}

## Pool of forecasting methods

\textcolor{red}{For the pool of methods, $P$ in Algorithm }  \@ref(alg:algo-lab), we consider nine methods implemented in the `forecast` [@forecast] package R package. They are (the specific R calls for fitting the models are given): 

i) automated ARIMA algorithm (`auto.arima`), 

ii) automated ETS algorithm (`ets`),

iii) feed-forward neural network with a single hidden layer fitted to the lags. The number of lags is automatically selected (`nnetar`),

iv) random walk with drift (`rwf` with drift=TRUE),

v) TBATS model (`tbats`), 

vi) Theta method (`thetaf`), 

vii) naive (`naive`), 

viii) STLM-AR Seasonal and Trend decomposition using Loess with AR modeling of the seasonally adjusted series (`stlm` with model function `ar`) and 

ix) seasonal naive (`snaive`).

In all cases, the default setting is used. Further, in the case of an error when fitting the series (e.g. a series is constant), the SNAIVE forecast method is used instead.
 
<!--It is worthy to emphasize that we used the default parameters in most methods without any hand tunning. Only in `auto.arima` we specified a more thorough search for hyperparameters than its default version.--> 

\section{Forecast-error measure}
\textcolor{red}{
The forecasting error measure ($E$ in Algorithm \@ref(alg:algo-lab)) is adapted from the Overall Weighted Average error described in the M4 competition guide, which adds together the Mean Absolute Scaled Error and the symmetric Mean Absolute Percentage Error. For each series and method, the Mean Absolute Scaled Error and the symmetric Mean Absolute Percentage Error are divided by the respective error of the Naive 2 method *over all series in the dataset* (i.e. MASE by the average MASE of Naive 2), and then added.
}

## Metal-learning model implementation
\todo{again, find a better name?}

We choose the gradient tree boosting model of **xgboost** (@chen2016xgboost) as the underlying implementation of the learning model due to following reasons:

 1. Ability to customize the model to fit our objective function.
 2. Computational efficiency: The size of the M4 data set \textcolor{red}{require} a efficient model.
 3. Good performance in structure based problems, state of the art model in machine learning challenges (@chen2016xgboost).
 
<!-- https://blog.cambridgespark.com/getting-started-with-xgboost-3ba1488bb7d4 -->
### The xgboost objective function

The basic boost algorithm produces numeric values from the features, one for each forecasting method in our pool. We apply the \textcolor{red}{softmax} transform to these values prior to computing the the objective function shown in Algorithm \@ref(alg:algo-lab) step 7, implemented as a custom objective function.

\textcolor{red}{xgboost requires a gradient and hessian of the objective function to fit the model.} The *correct* hessian is prone to numerical problems that need to be addressed for \textcolor{red}{boosting} to converge. This is a relative common problem and one simple fix is to use an upper bound of the hessian by clamping small values to a larger one. We computed a \textcolor{red}{different} upper bound of the hessian by removing some terms from the correct hessian. Although both alternatives converge, the later works faster, requiring less boosting steps to converge. This not only increases the computational efficiency, it also generalizes better due to a less complex set of trees produced in the final solution.

The  general meta-learning algorithm in \@ref(alg:algo-lab) is instanciated to:

     
  * $p(\bm{f}_j)$ is the output of the xgboost algorithm for the features extrated from series $X_j$.
  * $y_j = \frac{exp(p(\bm{f}_j)_k)}{ \sum_k exp(p(\bm{f}_j)_k)}$ is the transformation to probabilities of the xgboost output by applying the softmax transform.
  * $e_{jk}$ is the OWA error measure of forecasting method $k$ in series $j$.
  * $L_j = \sum_{k=1}^K y_{jk} e_{jk}$ is the loss function, step 6 of Algorithm \@ref(alg:algo-lab).
  * $G_j = \frac{\partial{L}}{\partial{y_j}} = y_j(e_j -\sum_k e_ky_k)$ is the gradient of the loss function.
  * The hessian $H_j$ is approximated by our upper bound $\hat{H_j}$:
  $$ H_j = \frac{\partial{G_j}}{\partial{y_j}} \approx \hat{H}_j = y_j(e_j(1-y_j) - G_j)  $$

The functions $G$ and $\hat{H}$ are passed to the xgboost algorithm to minimize our objective function $L$.
    
### Hyperparameters
 
 The results of xgboost  are particularly dependent on its hyper-parameters such as learning rate, number of boosting steps, maximum complexity allowed for the trees or subsampling sizes. We limit the hyper-parameter search space based on some initial results and rules of thumb and explore it using Bayesian optimization, (implemented in the `rBayesianOptimization` R package [@rBayesianOptimization]) measuring performance on a 10% holdout version of the reference set. We picked the simplest hyper-parameter set from the top solutions of the exploration.


## Prediction Intervals

The M4 Competition also featured a sub competition over the accuracy of prediction intervals. For this part, we used a different approach. In order to compute the intervals we used the point forecast produced by our meta-learner as the center of the interval and computed the 95% bounds of the interval by a linear combination of the bounds of three forecasting methods: Thetaf, snaive and naive methods.
The coefficients for the linear combination were calculated in a data driven way also over the M4 database. The procedure is as follows:
\todo{maybe too verbose/redundant?}

  1. For the training period of each series:
    1. Compute the point forecast using FFORMA. \textcolor{red}{At this stage we do not use the full dataset for training FFORMA, since it would produce too acurate forecasts in the same dataset, leading to very small prediction intervals. we divide the M4 dataset in two parts, A and B. We train FFORMA on part A and use it to produce the point forecasts over the series of part B, and the reverse.}
    2. Compute the 95% *predicion radius* for the thetaf, snaive and naive, this is the difference between the 95% upper bound and the point forecast for each forecast horizon.
  2. For each forecast horizon:
    1. Find the coefficients that minimize the MSIS error, \textcolor{red}{(defined in the M4 Competition guide)}, of the interval with the FFORMA point forecast as center and a linear combination of the radiuses of thetaf, snaive and naive as radius. The minimization is done by gradient descent over the test period of the series.
    
This procedure produces a set of three coefficients for each prediction horizon in the M4 dataset and these coefficients will be the same independently of the series we want to forecast. *Unlike the point forecast, these coefficients are not restricted to be probabilities.*



# Results{#results}

We measure the impact of our contribution by comparing the average forecasting error produced by our proposed combination model to a reference implementation of a traditional classification approach, in which a model is trained to pick the best method, which will then be used for the final forecast.
We use the same set of features, the same pool of forecasting methods, the same underlying implementation (xgboost), and in both approaches we perform hyperparameter search prior to fitting the model using bayesian optimization. This way, we isolate the model differences.
In the M4 competition dataset for point forecast, our approach produces a **relative improvement of 10\%** over the reference, measured as $\frac{classifier\ error}{proposed\ method\ error}$.

<!--
#classif 0.8566705
#our combi 0.7770559 

 Maybe we can show here results about the metalearning, how weights evolve with series length. What is the difference between weightedaveraging and selection in our model? What happens if we remove individual forecasting methods? How robust are we against this? wHich features are more important? -->

# Discussion and Conclusions{#conclusion}

We have presented an algorithm for forecasting using weighted averaging of a set of models. These weights are probabilities and can also be interpreted as a model selection algorithm, if one picks the methods receiving largest probability. This can be useful for interpretability or computational reasons. The objective function of the learning model assigns probabilities to forecasting methods in order to minimize the forecasting error that would be produced if we picked the methods at random using these probabilities. This contrasts with how the final forecasts are produced, which is a weighted average. We explored minimizing the weighted average of the forecasts, but the results did not improve over the simple version.

One advantage of the approach is that it is independent of the forecasting error measure, forcasting errors enter the model as additional precalculated constants This allows FFORMA to adapt to arbitrary error measures when models that directly minimize them would be restricted, e.g. a non differentiable error.

The source code for FFORMA is available at \url{https://github.com/robjhyndman/M4metalearning}.

<!-- \textcolor{blue}{Fotios email: We would like, though, to see a short section that discusses the reasons behind the good performance of your method. } -->

<!-- ##Reasons behind the performance -->

<!--   1. As opposed to individual methods in our pool, it is being trained to the specific error in the M4 Competition. The methods in our pool do not minimize the OWA error, but the squared loss, while we generated probabilities based on the OWA error. -->
<!--   0. As opposed to individual selection methods, it exploits domain bias. -->
<!--    One method could be discarded in a series when the error is low, because on average it performs bad in the rest of the dataset, while individual methods would pick it up. " -->
<!--    3. Exploiting dependencies between time series. In the M4 competition, some time series are similar, which can be hypothesized as macroeconomic indicators of neighbouring countries, etc. This effect is explited in the metalearning model and the series would produce similar features, and the errors that the model minimizes are more accurate. -->
<!--    2. AS opossed to traditional classification meta-learning approaches: Take the individual errors into account. This works two ways, capturing similarity the results in methods to keep the model simple (. Capturing differences in time series to keep the model simple (difficult do not end up affecting the model much). -->
<!--   4. Compared to a naive averaging of individual methods, it exploits easily discardable methods. Many ensemble approaches work by averaging the output of individual methods. This is especially when the number of individual forecasting methods grows and allows us to include methods with radically different assumptions that would not be appropiate to average. e.g."for really short time series you really cant do better than naive, others just overfit" -->
<!--   5. As opposed to just holdout crossvalidation, our method performs better. If we allow the method to overfit, i will reproduce the results of the holddout crossvaliation error, but in our case it improves it. -->
<!--   6. Bias variance tradeoff - Picking best method has high variance, by using the relative errors we reduce it. -->
<!--   7. EXPERIMENT: Train with few data and see what happens. -->
<!--   8. EXPERIMENT: compared against a traditional classifier approach with xgboost -->
<!--  - Features importance, evolution of weights according to some features. -->
<!-- E.G. "we can see that our meta learning approaches successfully captures the preference of X method against Y method when the strength of the trend grows, (compared to the true preference using the test set)". -->
<!--   - Comparison of performance of meta-learning against crossvalidation error. -->
<!-- E.G "our method outperforms model selection using crossvalidation, showing the successful exploitation of the domain information" -->
<!--  - Robustness against removing features, removing methods in the pool. -->
<!-- E.G. "The meta-learning approach is robust against changes in the pool of methods and feature set, showing that they are not spurious results..." -->
<!--  - Performance of Selection vs Averaging using our weights -->
<!-- E.G "The relative loss of accuracy if we want to perform model selection instead of averaging with our model". -->
<!-- -Performance compared to naive averaging, or how much the weights deviate from equal weights, to give a sense of the impact of the meta learning. -->
<!-- These need no to overlap with the general M4 paper, i.e. we are not discussing the performance of our method in weekly or monthly series and other results that I suppose they will talk about, and we only mention accuracy in a relative sense. -->
<!-- I will write some draft and then we can edit from there. -->

# References
