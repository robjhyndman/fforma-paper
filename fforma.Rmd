---
title: "FFORMA: Feature-based Forecast Model Averaging"
author:
- familyname: Montero-Manso
  othernames: Pablo
  address: Department of Mathematics, University of A Coru√±a, Spain
  email: p.montero.manso@udc.es
  correspondingauthor: true
- familyname: Athanasopoulos
  othernames: George
  address: Department of Econometrics and Business Statistics, Monash University, Australia
  email: george.athanasopoulos@monash.edu
- familyname: Hyndman
  othernames: Rob J
  address: Department of Econometrics and Business Statistics, Monash University, Australia
  email: rob.hyndman@monash.edu
- familyname: Talagala
  othernames: Thiyanga S
  address: Department of Econometrics and Business Statistics, Monash University, Australia
  email: thiyanga.talagala@monash.edu
abstract: We propose an automated method for obtaining weighted forecast combinations using time series features. The proposed approach involves two phases. First, we use a collection of time series to train a meta-model to assign weights to various possible forecasting methods with the goal of minimizing the average forecasting loss obtained from a weighted forecast combination. The inputs to the meta-model are features extracted from each series. In the second phase, we forecast new series using a weighted forecast combination where the weights are obtained from our previously trained meta-model. Our method outperforms a simple forecast combination, and outperforms all of the most popular individual methods in the time series forecasting literature. The approach achieved second position in the M4 competition.
keywords: Time series features, Forecast combination, XGBoost, M4 competition, Meta-learning
wpnumber: 19/18
jelcodes: C10,C14,C22
blind: false
cover: true
toc: false
bibliography: references.bib
biblio-style: authoryear-comp
output:
  MonashEBSTemplates::workingpaper:
    fig_caption: yes
    fig_height: 5
    fig_width: 8
    includes:
      in_header: preamble.tex
    keep_tex: yes
    number_sections: yes
    citation_package: biblatex
---

\lfoot{}

<!-- OLD ABSTRACT: It is well known that ensemble approaches produce improvements over single methods in statistical learning. Nevertheless, when calculating predictions over a large dataset, computation time for the whole ensemble can be prohibitive, so individual model selection becomes the preferred approach. We present a method for combining forecasting models by posing it as a classification problem using features extracted from the time series. Unlike regular classification problems, we minimize the average forecast error of the selected method rather than the classification error. Not only does this address the aim of accurate forecasting, it also provides measures of relative method accuracy across the time series, and relative difficulty across time series. In contrast, a classic classification approach would give the same importance to all series and methods. The presented classifier is compared with state-of-the-art approaches to forecasting and time series classification. The results show an improvement of error over alternative approaches. These experiments allow us to show the relevance of both the feature set and the proposed optimization approach to several collections of time series. The scalability of the approach allows it to be applied to forecasting a large collection of time series. It can also be efficiently trained to tailor specific domains or datasets. -->

# Introduction {#intro}

There are essentially two general approaches for forecasting a time series: (i) generating forecasts from a single model; and (ii) combining forecasts from many models (forecast model averaging). There has been a vast literature on the latter motivated by the seminal work of @Bates1969-cu and followed by a plethora of empirical applications showing that combination forecasts are often superior to their individual counterparts [see, @Cle1989;@Timmermann06, for example]. Combining forecasts using a weighted average is considered a successful way of hedging against the risk of selecting a misspecified model. A major challenge is in selecting an appropriate set of weights, and many attempts to do this have been worse than simply using equal weights --- something that has become known as the ``forecast combination puzzle'' [see for example, @Smith2009]. We address the problem of selecting the weights by using a meta-learning algorithm based on time series features.

There have been several previous attempts to use time series features combined with meta-learning for forecasting, \textcolor{red}{ both for model selection and combination} [see for example @prudencio2004using; @LEMKE20102006; @KucEtAl2016; and @Kang2017]. Recently, @fforms proposed the FFORMS (Feature-based FORecast Model Selection) framework that uses time series features combined with meta-learning for forecast-model selection. That is, features are used to select a single forecasting model. In this paper, we build on this framework by using meta-learning to select the weights for a weighted forecast combination. All candidate forecasting methods are applied, and the weights to be used in combining them are chosen based on the features of each time series. We call this framework FFORMA (Feature-based FORecast Model Averaging). FFORMA resulted in the second most accurate point forecasts and prediction intervals amongst all competitors in the M4 competition.

The rest of the paper is organized as follows. In \autoref{methodology} we describe the FFORMA framework in a general sense. \autoref{M4application} gives the details of our implementation of FFORMA in the M4 competition for generating both point and interval forecasts. This includes the required preprocessing steps, the set of features and forecast methods, as well as the specific implementation of the meta-learning model. We show empirical evidence on the performance of the approach in \autoref{conclusion} by quantifying the difference between our proposed learning model and a traditional classifier approach. \autoref{conclusion} also provides some final remarks and conclusions.

# Methodology {#methodology}

## Intuition and overview of FFORMA

The objective of our meta-learning approach is to derive a set of weights to combine forecasts generated from a *pool of methods* (e.g., \naive, exponential smoothing, ARIMA, etc.). The FFORMA framework requires a set of time series we refer to as the *reference set*. Each time series in the reference set is divided into a training period and a test period. From the training period a set of *time series features* are calculated (e.g., length of time series, strength of trend, autocorrelations, etc.). These form the inputs to the meta-learning model. Each method in the pool is fitted to the training period, forecasts are generated over the test period, and *forecast errors* (the difference between actual and forecast values) are computed. From these, a summary forecast loss measure from a weighted combination forecast can be computed for any given set of weights.

The meta-learning model learns to produce weights for all methods in the pool, as a function of the features of the series to be forecasted, by minimizing this summary forecast loss measure. Once the model is trained, weights can be produced for a new series for which forecasts are required. It is assumed that the new series comes from a *generating process* that is similar to some of those that form the reference set.

A common meta-learning approach is to select the best method in the pool of methods for each series; i.e., the method that produces the smallest forecast loss. This approach transforms the problem into a traditional classification problem by setting the individual forecasting methods as the classes and the best method as the target class for each time series. However,
there may be other methods that produce similar forecast errors to the best method, so the specific class chosen is less important than the forecast error resulting from each method. Further, some time series are more difficult to forecast than others, and hence have more impact on the total forecast error. This information is lost if the problem is treated as classification.

Consequently, we do not train our meta-learning algorithm using a classification approach. Instead, we pose the problem as finding a function that assigns *weights* to each forecasting method, with the objective of minimizing the expected loss that would have been produced if the methods were picked at random using these weights as probabilities. These are the weights in our weighted forecast combination. This approach is more general than classification, and can be thought of as classification with *per class weights* (the forecast errors) that vary per instance, combined with *per instance weights* that assign more importance to some series.

A flowchart of the FFORMA forecasting process can be seen in \autoref{flowchart}.

![Flowchart of the FFORMA forecasting process](fforma-flowchart.png){#flowchart}

<!-- The meta-learning process can therefore be summarized the following way: -->

<!--   * $X_i$ the $i$-eth series on the regerence set. -->
<!--   * $e_{ij}$ the error the individual forecasting method $j$ makes on series $i$. -->
<!--   * $x_{i}$ the feature vector extracted from series $X_i$. -->
<!--   * $f$ is a function belonging to a hypothesis space $H$ (such as linear functions) that maps features to probabilities, such as $f(x)_j$ is the probabilitie assigned to the $j$-eth method in the pool. -->

<!-- $$ argmin_{f \in H} \sum_i \sum_j f(x_{i})_je_{ij} $$ -->

<!--  Given a new time series, a trained meta-learning algorithm produces probabilities for each of the forecasting methods considered in the pool. The final forecast can be computed from these probabilities in two-ways: -->

<!--  1. A weighted average of the individual forecast methods. -->
<!--  This tends to produce the best forecast on average, but requires computing the forecasts of all methods in out pool, which may be time consuming. -->
<!--  2. Selecting the method with the largest probability. This prevents the need to calculate all the forecast of the individual methods in our pool, but produces less acurate forecasts on average. -->

## Algorithmic description

The operation of the FFORMA framework comprises two phases: (1) the offline phase, in which we train a meta-learner; and (2) the online phase, in which we use the pre-trained meta-learner to identify forecast combination weights for a new series. Algorithm \@ref(alg:algo-lab) presents the pseudo-code of the proposed framework.

<!--
  1. The set of forecasting methods, the pool of methods.
  2. The set of features to be extracted from the time series
  3. A training set of time series, the reference set.
  4. The forecasting error measure, such as mean squared error.

The FFORMA framework works in two phases: an **offline** phase, when the meta-learned is trained and an **online** phase, when forecasts are produced.  describes the two phases. -->

\begin{algorithm}[!ht]
  \caption{The FFORMA framework: Forecast combination based on meta-learning}
  \label{alg:algo-lab}
  \begin{algorithmic}[1]
    \Statex \textsc{Offline phase: train the learning model}
    \Statex \textbf{Inputs:}
      \Statex \hspace{1cm}$\{x_1, x_2, \dots,x_N\}$: $N$ observed time series forming the reference set.
      \Statex \hspace{1cm}$F$: a set of functions for calculating time series features.
      \Statex \hspace{1cm}$M$: a set of forecasting methods in the pool, e.g., \naive, ETS, ARIMA, etc.
    \Statex \textbf{Output:}
      \Statex \hspace{1cm}\text{FFORMA meta-learner: A function from the extracted features to a set of $M$}
      \Statex \hspace{1cm}\text{weights, one for each forecasting method.}\vspace{0.3cm}
    \Statex \textit{Prepare the meta-data}\vspace{0.1cm}
    \For{$n=1$ to $N$:}
      \State Split $x_n$ into a training period and test period.
      \State Calculate the set of features $\bm{f}_n\in F$ over the training period.
      \State Fit each forecasting method $m\in M$ over the training period and generate
      \Statex\hspace*{.5cm} forecasts over the test period.
      \State Calculate forecast losses $L_{nm}$ over the test period.
    \EndFor\vspace{0.3cm}
    \Statex \textit{Train the meta-learner, $w$}\vspace{0.1cm}
            \State Train a learning model based on the meta-data and errors, by minimizing:
        \[
          \mathop{\text{argmin}}\limits_w \sum_{n=1}^N \sum_{m=1}^M w(\bm{f}_n)_{m} L_{nm}.
        \]
    \Statex
     \Statex \textsc{Online phase: forecast a new time series}
    \Statex \textbf{Input:}
    \Statex \hspace{1cm}\text{FFORMA meta-learner from offline phase}.
     \Statex \textbf{Output:}
      \Statex \hspace{1cm}\text{Forecast the new time series $x_{new}$}.
      \For{each $x_{new}$:}
        \State Calculate features $\bm{f}_{new}$ by applying $F$.
        \State Use the meta-learner to produce $\bm{w}(\bm{f}_{new})$ an $M$-vector of weights.
        \State Compute the individual forecasts of the $M$ forecasting methods in the pool.
        \State Combine individual forecasts using $\bm{w}$ to generate final forecasts.
      \EndFor
  \end{algorithmic}
\end{algorithm}

<!-- The algorithm produces weights for each forecasting method. These weights can also be interpreted as the probability of each method being best. These weights can be used either to select the "best" forecasting method for each series or to combine the forecasts using weighted linear combination. Note that the accuracy of the FFORMA meta-learner depends on three main factors: -->

<!--   1. Forecasting methods used in the pool -->
<!--   2. The set of time series features we considered -->
<!--   3. The collection of time series used to train the classifier. -->

<!--   Section 3 provides a more detailed description of application of the FFORMA framework over the course of M4 competition.  -->

# Implementation and application to the M4 competition {#M4application}

## Reference set

The M4 dataset includes 100,000 time series of yearly, quarterly, monthly, weekly, daily and hourly data. All 100,000 series form the reference set. Each series is split into a training period and a test period. The length of the test period for each time series was set to be equal to the forecast horizon set by the competition. Series with training periods comprising fewer than two observations, or series that were constant over the training period, were eliminated from the reference set.

\begin{table}[!p]
\centering\small\tabcolsep=0.1cm
\begin{spacing}{1.15}
\caption{Features used in FFORMA framework.}
\label{feature}
\begin{tabular}{@{}llp{8.7cm}cc@{}}
\toprule
\multicolumn{2}{l}{Feature} & Description & Non-seasonal & Seasonal\\
\midrule
1  & T                & length of time series                                                                                                                                    & \yes  & \yes \\
2  & trend            & strength of trend                                                                                                                                        & \yes  & \yes\\
3  & seasonality      & strength of seasonality                                                                                                                                  & -     & \yes \\
4  & linearity        & linearity                                                                                                                                                & \yes  & \yes \\
5  & curvature        & curvature                                                                                                                                                & \yes  & \yes \\
6  & spikiness        & spikiness                                                                                                                                                & \yes  & \yes \\
7  & e\_acf1          & first ACF value of remainder series                                                                                                                      & \yes  & \yes \\
8  & e\_acf10         & sum of squares of first 10 ACF values of remainder series                                                                                                & \yes  & \yes \\
9  & stability        & stability                                                                                                                                                & \yes  & \yes \\
10 & lumpiness        & lumpiness                                                                                                                                                & \yes  & \yes \\
11 & entropy          & spectral entropy                                                                                                                                         & \yes  & \yes \\
12 & hurst            & Hurst exponent                                                                                                                                           & \yes  & \yes \\
13 & nonlinearity     & nonlinearity                                                                                                                                             & \yes\ & \yes \\
13 & alpha            & ETS(A,A,N) $\hat\alpha$                                                                                                                                  & \yes  & \yes \\
14 & beta             & ETS(A,A,N) $\hat\beta$                                                                                                                                   & \yes  & \yes\\
15 & hwalpha          & ETS(A,A,A) $\hat\alpha$                                                                                                                                  & -     & \yes \\
16 & hwbeta           & ETS(A,A,A) $\hat\beta$                                                                                                                                   & -     & \yes \\
17 & hwgamma          & ETS(A,A,A) $\hat\gamma$                                                                                                                                  & -     & \yes \\
18 & ur\_pp           & test statistic based on Phillips-Perron test                                                                                                             & \yes  & \yes \\
19 & ur\_kpss         & test statistic based on KPSS test                                                                                                                        & \yes  & \yes \\
20 & y\_acf1          & first ACF value of the original series                                                                                                                   & \yes  & \yes \\
21 & diff1y\_acf1     & first ACF value of the differenced series                                                                                                                & \yes  & \yes \\
22 & diff2y\_acf1     & first ACF value of the twice-differenced series                                                                                                          & \yes  & \yes \\
23 & y\_acf10         & sum of squares of first 10 ACF values of original series                                                                                                 & \yes  & \yes \\
24 & diff1y\_acf10    & sum of squares of first 10 ACF values of differenced series                                                                                              & \yes  & \yes \\
25 & diff2y\_acf10    & sum of squares of first 10 ACF values of twice-differenced series                                                                                        & \yes  & \yes \\
26 & seas\_acf1       & autocorrelation coefficient at first seasonal lag                                                                                                        & -     & \yes \\
27 & sediff\_acf1     & first ACF value of seasonally differenced series                                                                                                         & -     & \yes\\
28 & y\_pacf5         & sum of squares of first 5 PACF values of original series                                                                                                 & \yes  & \yes \\
29 & diff1y\_pacf5    & sum of squares of first 5 PACF values of differenced series                                                                                              & \yes  & \yes \\
30 & diff2y\_pacf5    & sum of squares of first 5 PACF values of twice-differenced series                                                                                        & \yes  & \yes \\
31 & seas\_pacf       & partial autocorrelation coefficient at first seasonal lag                                                                                                & \yes  & \yes \\
32 & crossing\_point  & number of times the time series crosses the median                                                                                                       & \yes  & \yes \\
33 & flat\_spots      & number of flat spots, calculated by discretizing the series into 10 equal sized intervals and counting the maximum run length within any single interval & \yes  & \yes \\
34 & nperiods         & number of seasonal periods in the series                                                                                                                 & -     & \yes \\
35 & seasonal\_period & length of seasonal period                                                                                                                                & -     & \yes \\
36 & peak             & strength of peak                                                                                                                                         & \yes  & \yes \\
37 & trough           & strength of trough                                                                                                                                       & \yes  & \yes \\
38 & ARCH.LM          & ARCH LM statistic                                                                                                                                        & \yes  & \yes \\
39 & arch\_acf        & sum of squares of the first 12 autocorrelations of $z^2$                                                                                                 & \yes  & \yes \\
40 & garch\_acf       & sum of squares of the first 12 autocorrelations of $r^2$                                                                                                 & \yes  & \yes \\
41 & arch\_r2         & $R^2$ value of an AR model applied to $z^2$                                                                                                              & \yes  & \yes \\
42 & garch\_r2        & $R^2$ value of an AR model applied to $r^2$                                                                                            & \yes  & \yes \\
\bottomrule
\end{tabular}
\end{spacing}
\end{table}

## Time series features

\autoref{feature} provides a brief description of the features used in this experiment, $F$ in Algorithm \@ref(alg:algo-lab). The functions to calculate these are implemented in the `tsfeatures` R package by @tsfeatures. Most of the features (or variations of these) have been previously used in a forecasting context by @hyndman2015large and @fforms, and are described in more detail there. The ARCH.LM statistic was calculated based on the Lagrange Multiplier test of @engle1982autoregressive for autoregressive conditional heteroscedasticity (ARCH). The heterogeneity features 39--42 are based on two computed time series: the original time series is pre-whitened using an AR model resulting in $z$; a GARCH(1,1) model is then fitted to $z$ to obtain the residual series, $r$.

Features corresponding only to seasonal time series are set to zero for non-seasonal time series. For the sake of generality, we have not used any of the domain-specific features such as macro, micro, finance, etc., even though this information was available in the M4 data set.

<!--No exogenous features were used, even though they were available in the M4 dataset, such as which domain the series belongs to (e.g. macroeconomics, finance, tourism...).-->

## Pool of forecasting methods

We considered eight methods implemented in the `forecast` package in R [@forecast] for the pool of methods, $P$ in Algorithm \@ref(alg:algo-lab):

1. \naive\ (`naive`);
1. random walk with drift (`rwf` with drift=TRUE);
1. seasonal \naive\ (`snaive`).
1. theta method (`thetaf`);
1. automated ARIMA algorithm (`auto.arima`);
1. automated exponential smoothing algorithm (`ets`);
1. TBATS model (`tbats`);
1. STLM-AR Seasonal and Trend decomposition using Loess with AR modeling of the seasonally adjusted series (`stlm` with model function `ar`).

The R functions are given in parentheses. In all cases, the default settings are used. If any function returned an error when fitting the series (e.g. a series is constant), the `snaive` forecast method was used instead.

<!--It is worthy to emphasize that we used the default parameters in most methods without any hand tunning. Only in `auto.arima` we specified a more thorough search for hyper-parameters than its default version.-->

## Forecast loss measure

The forecasting loss, $L$ in Algorithm \@ref(alg:algo-lab), was adapted from the Overall Weighted Average (OWA) error described in the M4 competitor's guide @M4compguide, which adds together the Mean Absolute Scaled Error and the symmetric Mean Absolute Percentage Error. For each series and method, the Mean Absolute Scaled Error and the symmetric Mean Absolute Percentage Error were divided by the respective error of the Naive 2 method *over all series in the dataset* (i.e., MASE by the average MASE of Naive 2), and then added.

## Meta-learning model implementation

We used the gradient tree boosting model of `xgboost` as the underlying implementation of the learning model [@chen2016xgboost]. This is a state-of-the-art model that is computationally efficient and has shown good performance in structure based problems. The great advantage of its application here is that we are able to customise the model with our specific objective function.

The basic `xgboost` algorithm produces numeric values from the features, one for each forecasting method in our pool. We applied the softmax transform to these values prior to computing the objective function. This was implemented as a *custom objective function* in the `xgboost` framework.

`xgboost` requires a gradient and hessian of the objective function to fit the model. The *correct* hessian is prone to numerical problems that need to be addressed for the boosting to converge. This is a relatively common problem and one simple fix is to use an upper bound of the hessian by clamping its small values to a larger one. We computed a different upper bound of the hessian by removing some terms from the correct hessian. Although both alternatives converged, the latter worked faster, requiring less boosting steps to converge. This not only increased the computational efficiency, it also generalized better due to a less complex set of trees produced in the final solution.

The general parameters of the meta-learning in Algorithm \@ref(alg:algo-lab) were set as follows.

  * $p(\bm{f}_n)_m$ is the output of the `xgboost` algorithm corresponding to forecasting method $m$, based on the features extracted from series $x_n$.
  * $w(\bm{f}_n)_{m} = \frac{\exp(p(\bm{f}_n)_m)}{ \sum_m \exp(p(\bm{f}_n)_m)}$ is the transformation to weights of the `xgboost` output by applying the softmax transform.
  * $L_{nm}$ is the contribution to the OWA error measure of method $m$ for the series $n$.
  * $\bar{L}_n = \sum_{m=1}^M w(\bm{f}_n)_m L_{nm}$ is the weighted average loss function.
  * $G_{nm} = \frac{\partial{\bar{L}_n}}{\partial{p(\bm{f}_n)_m}} = w_{nm}(L_{nm} - \bar{L}_n)$ is the gradient of the loss function.
  * The hessian $H_{nm}$ was approximated by our upper bound $\hat{H}_{nm}$:
    \[
      H_{nm} = \frac{\partial{G_n}}{\partial{p(\bm{f}_n)_m}} \approx \hat{H}_n = w_n(L_n(1-w_n) - G_n)
    \]

The functions $G$ and $\hat{H}$ were passed to `xgboost` to minimize the objective function $\bar{L}$.

The results of `xgboost` are particularly dependent on its hyper-parameters such as learning rate, number of boosting steps, maximum complexity allowed for the trees or sub-sampling sizes. We limited the hyper-parameter search space based on some initial results and rules-of-thumb and explored it using Bayesian optimization [implemented in the R package `rBayesianOptimization`, @rBayesianOptimization] measuring performance on a 10% holdout version of the reference set. We picked the simplest hyper-parameter set from the top solutions of the exploration.

## Prediction intervals

For each series $x_{new}$, we used as the centre of the interval the point forecast produced by our meta-learner. Then the 95% bounds of the interval were generated by a linear combination of the bounds of three forecasting methods: \naive, theta and seasonal \naive. These methods were chosen from the initial pool purely for saving computational time. The whole pool of methods should be used if computational time is not a constraint. The coefficients for the linear combination were calculated in a data-driven way over the M4 database. The complete procedure was as follows:

1. We divided the M4 dataset into two parts: A and B. We trained the FFORMA learner using the training periods of the series in part A and produced point forecasts over the test periods of the series of part B, and vice versa. This partitioning prevents overfitting caused from having point forecasts that are extremely accurate if the same dataset was used for both center and the interval bounds.

2. We computed the 95% *prediction interval radius* for the \naive, theta, and seasonal \naive\ methods. This is the difference between the 95% upper or lower bound and the point forecast for each forecast horizon, as we assume the intervals to be symmetric around the point forecast.

3. For each forecast horizon we found the coefficients that minimized the MSIS of the interval, as defined in the M4 Competitor's guide [@M4compguide], with the FFORMA point forecast as the centre and a linear combination of the radii of \naive, theta, seasonal \naive\ forecasts as the interval. The minimization was done by gradient descent over the test period of the series.

This method produced a set of three coefficients for each prediction horizon in the M4 dataset and these coefficients were the same independently of the series we want to forecast. Unlike the point forecasts, these coefficients were not restricted to be probabilities.

# Discussion and conclusions {#conclusion}

We have presented an algorithm for forecasting using weighted averaging of a set of models. The objective function of the learning model assigns weights to forecasting methods in order to minimize the forecasting error that would be produced if we picked the methods at random using these weights as probabilities. This contrasts with how the final forecasts are produced, which is a weighted average, not a selection.

These weights can however be used as part of a model selection algorithm, if one picks the method receiving the largest weight. This can be useful for interpretability or computational reasons, at the cost of forecasting performance.

In order to evaluate the impact of our contribution, we compared the average forecast error produced by FFORMA with a model selection approach. All implementation details were kept the same as in FFORMA; specifically we used the same set of features, the same pool of forecasting methods, the same underlying implementation (`xgboost`) but with a standard cross-entropy loss, and the same hyper-parameter search. This enabled us to measure the impact of the FFORMA loss function against a model selection approach, all other things being equal. We applied both FFORMA and the model selection approach to the M4 dataset and compared their overall point forecast errors. \textcolor{red}{ The average OWA error of the model selection approach was 10\% larger than FFORMA. This improvement is entirely due to the proposed model averaging rather than model selection. On the other hand, FFORMA deviates significantly from simple averaging. The latter produces a 14\% increase in error for the same pool of methods. A simple cluster analysis of the weights shows that roughly 40\% of the time series receive a weight profile similar to equal weights (a simple average) while for the remaining 60\% of the series, one of the methods in the pool is clearly dominant.} 

\textcolor{red}{FFORMA is also robust against changes in the pool of forecasting methods. The maximum increase in error when removing a single method from the original pool of nine methods is 1\%. This maximum occurs when removing the Random Walk with Drift, which receives an average of 15\% weight accross all series in the M4. Also, the removal of any method results in an increased error compared to the original pool. Hence, there are no methods in the pool that impact the error negatively, even though individually some methods perform much worse than others when averaged over all the series. This suggests that the FFORMA algorithm is able to assign correct weights to the forecasting methods that are specialized for a specific type of series.} 

\textcolor{red}{Therefore, we believe that the good performance of the FFORMA can be attributed to: (i) the specific set of features and learning model (`xgboost`);
(ii) the weights allocated by FFORMA that can range from a simple averaging (equal weights), to a profile that assigns most of the importance to only a single method. This allows FFORMA to outperform both simple averaging and method selection alternatives; and (iii) FFORMA adapts to the methods in the pool, making the specific pool of methods less critical than in other combination approaches.}

<!--
#classif 0.8566705
#our combi 0.7770559

 Maybe we can show here results about the metalearning, how weights evolve with series length. What is the difference between weightedaveraging and selection in our model? What happens if we remove individual forecasting methods? How robust are we against this? wHich features are more important? -->

One advantage of our approach is that its form is independent of the forecasting loss measure. Forecast errors enter the model as additional pre-calculated values. This allows FFORMA to adapt to arbitrary loss functions when models that directly minimize them would be restricted. For example, our approach can be applied to non-differentiable errors.

<!-- We explored minimizing the weighted average of the forecasts, but the results did not improve over the simple version.
 -->

The source code for FFORMA is available at [github.com/robjhyndman/M4metalearning](https://github.com/robjhyndman/M4metalearning).

<!-- \textcolor{blue}{Fotios email: We would like, though, to see a short section that discusses the reasons behind the good performance of your method. } -->

<!-- ##Reasons behind the performance -->

<!--   1. As opposed to individual methods in our pool, it is being trained to the specific error in the M4 competition. The methods in our pool do not minimize the OWA error, but the squared loss, while we generated probabilities based on the OWA error. -->
<!--   0. As opposed to individual selection methods, it exploits domain bias. -->
<!--    One method could be discarded in a series when the error is low, because on average it performs bad in the rest of the dataset, while individual methods would pick it up. " -->
<!--    3. Exploiting dependencies between time series. In the M4 competition, some time series are similar, which can be hypothesized as macroeconomic indicators of neighbouring countries, etc. This effect is explited in the metalearning model and the series would produce similar features, and the errors that the model minimizes are more accurate. -->
<!--    2. AS opossed to traditional classification meta-learning approaches: Take the individual errors into account. This works two ways, capturing similarity the results in methods to keep the model simple (. Capturing differences in time series to keep the model simple (difficult do not end up affecting the model much). -->
<!--   4. Compared to a naive averaging of individual methods, it exploits easily discardable methods. Many ensemble approaches work by averaging the output of individual methods. This is especially when the number of individual forecasting methods grows and allows us to include methods with radically different assumptions that would not be appropiate to average. e.g."for really short time series you really cant do better than naive, others just overfit" -->
<!--   5. As opposed to just holdout crossvalidation, our method performs better. If we allow the method to overfit, i will reproduce the results of the holddout crossvaliation error, but in our case it improves it. -->
<!--   6. Bias variance tradeoff - Picking best method has high variance, by using the relative errors we reduce it. -->
<!--   7. EXPERIMENT: Train with few data and see what happens. -->
<!--   8. EXPERIMENT: compared against a traditional classifier approach with xgboost -->
<!--  - Features importance, evolution of weights according to some features. -->
<!-- E.G. "we can see that our meta learning approaches successfully captures the preference of X method against Y method when the strength of the trend grows, (compared to the true preference using the test set)". -->
<!--   - Comparison of performance of meta-learning against crossvalidation error. -->
<!-- E.G "our method outperforms model selection using crossvalidation, showing the successful exploitation of the domain information" -->
<!--  - Robustness against removing features, removing methods in the pool. -->
<!-- E.G. "The meta-learning approach is robust against changes in the pool of methods and feature set, showing that they are not spurious results..." -->
<!--  - Performance of Selection vs Averaging using our weights -->
<!-- E.G "The relative loss of accuracy if we want to perform model selection instead of averaging with our model". -->
<!-- -Performance compared to naive averaging, or how much the weights deviate from equal weights, to give a sense of the impact of the meta learning. -->
<!-- These need no to overlap with the general M4 paper, i.e. we are not discussing the performance of our method in weekly or monthly series and other results that I suppose they will talk about, and we only mention accuracy in a relative sense. -->
<!-- I will write some draft and then we can edit from there. -->

# References
